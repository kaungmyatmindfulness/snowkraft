[
  {
    "id": 501,
    "questionText": "A data engineer needs to create a table from Parquet files in a stage where the schema is unknown. They want Snowflake to automatically detect the column names and data types from the files. Which SQL pattern correctly accomplishes this?",
    "questionType": "single",
    "explanation": "The INFER_SCHEMA table function detects column definitions from staged semi-structured files (Parquet, Avro, ORC, JSON). The correct pattern combines three elements: (1) INFER_SCHEMA() to detect columns, (2) ARRAY_AGG(OBJECT_CONSTRUCT(*)) to format the schema as a template, and (3) CREATE TABLE ... USING TEMPLATE to create the table structure.\n\nWhy other options fail:\n\u2022 Option A (CLONE @stage) is invalid syntax\u2014CLONE works on tables/schemas/databases, not stages. SCHEMA_EVOLUTION applies to existing tables during COPY INTO, not table creation.\n\u2022 Option B (SELECT * FROM @stage) creates a table via CTAS but requires knowing the schema beforehand\u2014it doesn't auto-detect column names and types from the file metadata.\n\u2022 Option D (VARIANT column) loads all data into a single VARIANT column, losing the structured schema\u2014this is the opposite of schema detection.\n\nKey exam tip: INFER_SCHEMA requires a named FILE_FORMAT object (like 'my_parquet_format'), not a type string like 'parquet'. The FILE_FORMAT must already exist before calling INFER_SCHEMA.",
    "domain": "4",
    "topic": "Schema Detection",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "CREATE TABLE t CLONE @stage WITH SCHEMA_EVOLUTION = TRUE",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2889
      },
      {
        "answerText": "CREATE TABLE t AS SELECT * FROM @stage (FILE_FORMAT => 'parquet')",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2886
      },
      {
        "answerText": "CREATE TABLE t USING TEMPLATE (SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*)) FROM TABLE(INFER_SCHEMA(LOCATION=>'@stage', FILE_FORMAT=>'my_parquet_format')))",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2887
      },
      {
        "answerText": "CREATE TABLE t (data VARIANT); COPY INTO t FROM @stage FILE_FORMAT = (TYPE = PARQUET)",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2888
      }
    ]
  },
  {
    "id": 502,
    "questionText": "An organization on Snowflake Standard Edition accidentally dropped a table containing critical financial data. The DROP occurred 36 hours ago. The table was created with default settings. What is the correct recovery approach?",
    "questionType": "single",
    "explanation": "Understanding the data lifecycle is critical: Time Travel \u2192 Fail-safe \u2192 Gone. In Standard Edition, permanent tables have a maximum DATA_RETENTION_TIME_IN_DAYS of 1 day (this is both the default and the maximum). Since 36 hours exceeds 24 hours, the data has exited Time Travel but entered Fail-safe, which lasts 7 additional days.\n\nWhy other options fail:\n\u2022 Option A (UNDROP TABLE) only works during the Time Travel period. After 24 hours on Standard Edition, UNDROP is no longer available\u2014the table's Time Travel data has expired.\n\u2022 Option C (AT OFFSET => -129600) attempts to query 36 hours ago (129,600 seconds), but Time Travel queries also respect the retention period\u2014this would fail with an error.\n\u2022 Option D (impossible) is wrong because Fail-safe exists precisely for this scenario\u2014it's the last-resort recovery mechanism.\n\nKey exam tips: (1) Only Snowflake Support can recover from Fail-safe\u2014it's not self-service. (2) Fail-safe applies only to permanent tables, not transient or temporary. (3) Enterprise Edition extends Time Travel to 90 days maximum, but Fail-safe is always 7 days regardless of edition.",
    "domain": "6",
    "topic": "Time Travel",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Contact Snowflake Support for Fail-safe recovery",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 2891
      },
      {
        "answerText": "Recovery is impossible - Standard Edition lacks data protection",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2893
      },
      {
        "answerText": "Use SELECT ... AT(OFFSET => -129600) to query historical data",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2892
      },
      {
        "answerText": "Execute UNDROP TABLE since Enterprise Edition is not required for Time Travel",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2890
      }
    ]
  },
  {
    "id": 503,
    "questionText": "A query returns results in under 100 milliseconds without consuming any warehouse credits. Which Snowflake component is MOST likely responsible for this behavior?",
    "questionType": "single",
    "explanation": "The Cloud Services layer handles metadata operations and caches that can return results without warehouse compute. Queries returning in under 100ms with zero credits are served from either the metadata cache (for COUNT(*), MIN/MAX on clustering keys) or the result cache (for identical previously-executed queries). The Cloud Services layer is the 'brain' of Snowflake - managing query optimization, metadata storage, authentication, and access control.\n\nWhy other options fail:\n\u2022 Local disk cache warming on compute cluster - This cache (warehouse cache) stores raw data from cloud storage on local SSDs. It still requires warehouse compute and credits to process the cached data, so it would not result in zero credit consumption.\n\u2022 Automatic query parallelization across micro-partitions - Parallelization is a compute optimization that requires an active warehouse. It makes queries faster but still consumes credits proportional to the work performed.\n\u2022 Storage layer's columnar compression optimization - Compression reduces storage costs and I/O, but reading compressed data still requires warehouse compute. This optimization affects storage efficiency, not credit-free query execution.\n\nKey exam tip: Zero credits + sub-second response = Cloud Services layer (metadata or result cache). If any bytes are scanned, warehouse compute was used.",
    "domain": "1",
    "topic": "Architecture",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Local disk cache warming on the compute cluster",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2894
      },
      {
        "answerText": "Automatic query parallelization across micro-partitions",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2895
      },
      {
        "answerText": "Cloud Services layer metadata cache or result cache",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2896
      },
      {
        "answerText": "Storage layer's columnar compression optimization",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2897
      }
    ]
  },
  {
    "id": 504,
    "questionText": "A warehouse is configured with AUTO_SUSPEND = 300 and AUTO_RESUME = TRUE. The warehouse has been running queries continuously for 2 hours and is then suspended. When a new query arrives 10 minutes later, what is the expected behavior regarding local disk cache?",
    "questionType": "single",
    "explanation": "When a warehouse is suspended, all compute resources are released, including the local SSD cache (warehouse cache/remote disk cache). This cache stores micro-partition data retrieved from cloud storage to accelerate subsequent queries. Upon resumption, the warehouse spins up fresh compute nodes with empty caches, requiring data to be re-fetched from cloud storage. This 'cold start' effect means initial queries after resume typically run slower until the cache warms up through normal query activity.\n\nWhy other options fail:\n\u2022 Cache persists for 24 hours after suspension - This is incorrect. There is no time-based cache persistence after suspension. Warehouse resources, including cache, are completely released when suspended to stop billing. The 24-hour retention applies to result cache in Cloud Services, not warehouse disk cache.\n\u2022 Cache is preserved in Cloud Services layer and restored on resume - The Cloud Services layer maintains the result cache (query results) and metadata cache, but NOT the warehouse disk cache. The local disk cache exists only on warehouse compute nodes.\n\u2022 Cache persists but queries queue until manual cache validation - This describes no real Snowflake behavior. There is no manual cache validation process, and caches do not persist across suspension cycles.\n\nKey exam tip: Warehouse suspension = all compute resources released = cache lost. This is why some teams keep warehouses running during business hours (using AUTO_SUSPEND wisely) for cache benefits.",
    "domain": "3",
    "topic": "Virtual Warehouses",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Cache persists but queries queue until manual cache validation",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2901
      },
      {
        "answerText": "Cache is preserved in the Cloud Services layer and restored on resume",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2900
      },
      {
        "answerText": "Cache persists for 24 hours after suspension and will be available",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2898
      },
      {
        "answerText": "Cache is lost on suspend; queries will need to re-fetch data from cloud storage",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 2899
      }
    ]
  },
  {
    "id": 505,
    "questionText": "A data engineer is loading CSV files from an S3 external stage. The files have inconsistent column ordering across different uploads, but all files contain the same column names in their headers. Which COPY INTO option ensures correct data mapping regardless of column position?",
    "questionType": "single",
    "explanation": "MATCH_BY_COLUMN_NAME maps source columns to target columns by name rather than ordinal position. This is essential when source files have varying column orders but consistent column names. For CSV files, you must also set PARSE_HEADER = TRUE so Snowflake can read column names from the first row. Values can be CASE_SENSITIVE, CASE_INSENSITIVE, or NONE (default, uses positional mapping). This option also works natively with self-describing formats like Parquet, Avro, and ORC.\n\nWhy other options fail:\n\u2022 ENFORCE_LENGTH = FALSE - This option controls whether to truncate strings that exceed the target column's length. It has nothing to do with column ordering or name matching. It addresses data type/length issues, not structural mapping.\n\u2022 ON_ERROR = CONTINUE - This error-handling option determines behavior when rows fail to load (skip the row and continue). It does not affect how columns are mapped; it just handles load errors after mapping is attempted.\n\u2022 FORCE = TRUE with column transformations - FORCE = TRUE reloads files that were previously loaded (bypassing load history). Column transformations in the SELECT clause can reorder columns, but this requires manual specification and doesn't dynamically match by name.\n\nKey exam tip: MATCH_BY_COLUMN_NAME + PARSE_HEADER = TRUE is the standard pattern for CSV files with headers and inconsistent column ordering.",
    "domain": "4",
    "topic": "Data Loading",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "ENFORCE_LENGTH = FALSE",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2902
      },
      {
        "answerText": "MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 2904
      },
      {
        "answerText": "FORCE = TRUE with column transformations",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2905
      },
      {
        "answerText": "ON_ERROR = CONTINUE",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2903
      }
    ]
  },
  {
    "id": 506,
    "questionText": "A security administrator needs to configure a resource monitor that sends alerts at 75% and suspends warehouses at 100% of the credit quota. Which role has the MINIMUM privileges required to create this resource monitor and assign it to specific warehouses?",
    "questionType": "single",
    "explanation": "Only ACCOUNTADMIN can create resource monitors in Snowflake. Resource monitors are account-level objects that control credit consumption across warehouses. Creating them requires the global CREATE RESOURCE MONITOR privilege, which only ACCOUNTADMIN possesses by default. After creation, ACCOUNTADMIN can grant MODIFY privilege (to change settings) and MONITOR privilege (to view usage) on the resource monitor to other roles for ongoing management.\n\nWhy other options fail:\n\u2022 SYSADMIN - can manage all database and warehouse objects - SYSADMIN is designed for managing data objects (databases, schemas, tables) and warehouses, but resource monitors are account-level administrative objects outside SYSADMIN's scope. SYSADMIN cannot create resource monitors.\n\u2022 SECURITYADMIN - handles all security-related configurations - SECURITYADMIN manages users, roles, and grants (security principals and privileges). Resource monitors are cost governance objects, not security objects. SECURITYADMIN has no privileges for resource monitor creation.\n\u2022 ORGADMIN - manages organization-wide resource allocation - ORGADMIN operates at the organization level (managing multiple Snowflake accounts), not within a single account. Resource monitors are account-scoped objects. ORGADMIN cannot create objects within individual accounts.\n\nKey exam tip: Resource monitors = ACCOUNTADMIN only for creation. Remember the hierarchy: ORGADMIN (org) > ACCOUNTADMIN (account) > SECURITYADMIN/SYSADMIN (security/objects).",
    "domain": "3",
    "topic": "System Roles",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "SECURITYADMIN - handles all security-related configurations",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2908
      },
      {
        "answerText": "ACCOUNTADMIN - resource monitors require account-level privileges",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 2907
      },
      {
        "answerText": "SYSADMIN - can manage all database and warehouse objects",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2906
      },
      {
        "answerText": "ORGADMIN - manages organization-wide resource allocation",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2909
      }
    ]
  },
  {
    "id": 507,
    "questionText": "A development team needs tables for ETL staging data that persists across sessions but doesn't require disaster recovery capabilities. They want to minimize storage costs. Which combination of table type and settings achieves this with the LOWEST total storage footprint?",
    "questionType": "single",
    "explanation": "Transient tables with DATA_RETENTION_TIME_IN_DAYS = 0 provide the lowest storage footprint for persistent data. Transient tables eliminate Fail-safe entirely (permanent tables always have 7 days of Fail-safe storage you cannot disable). By also setting Time Travel retention to 0, you eliminate both historical data storage mechanisms. This means only current data is stored - no additional copies for recovery. This is ideal for ETL staging data that can be regenerated.\n\nWhy other options fail:\n\u2022 Permanent table with DATA_RETENTION_TIME_IN_DAYS = 0 - Even with Time Travel disabled, permanent tables ALWAYS have 7 days of Fail-safe. This mandatory Fail-safe period stores data for disaster recovery and cannot be removed. For a 100GB table, that's potentially 100GB+ of Fail-safe storage you cannot avoid.\n\u2022 Temporary table (session-scoped, auto-dropped) - Temporary tables have no Fail-safe and can have 0 Time Travel, matching transient tables for storage efficiency. However, they are session-scoped and automatically dropped when the session ends. The requirement states data must 'persist across sessions,' which temporary tables cannot do.\n\u2022 External table pointing to cloud storage - External tables don't store data in Snowflake (just metadata pointing to external files). While this minimizes Snowflake storage, the question asks about loading data INTO Snowflake, and external tables are read-only references, not true data storage.\n\nKey exam tip: For minimum storage: Transient + 0 Time Travel = no Fail-safe + no Time Travel. Permanent tables ALWAYS have 7-day Fail-safe regardless of settings.",
    "domain": "1",
    "topic": "Table Types",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Transient table with DATA_RETENTION_TIME_IN_DAYS = 0",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 2911
      },
      {
        "answerText": "Temporary table (session-scoped, auto-dropped)",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2912
      },
      {
        "answerText": "Permanent table with DATA_RETENTION_TIME_IN_DAYS = 0",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2910
      },
      {
        "answerText": "External table pointing to cloud storage",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2913
      }
    ]
  },
  {
    "id": 508,
    "questionText": "A multi-cluster warehouse is configured with MIN_CLUSTER_COUNT = 2, MAX_CLUSTER_COUNT = 6, and SCALING_POLICY = STANDARD. During a period of high concurrency, all 6 clusters are running. Which statement accurately describes what happens when query load decreases?",
    "questionType": "single",
    "explanation": "With STANDARD scaling policy, Snowflake scales clusters up immediately when queries queue, and scales down promptly when load decreases. The key constraint is MIN_CLUSTER_COUNT - the warehouse will never run fewer clusters than this minimum, even during idle periods. In this scenario, as queries complete and load drops, clusters scale down from 6 toward 2, but stop at 2 (the minimum). These 2 clusters remain running until AUTO_SUSPEND timeout, at which point the entire warehouse suspends.\n\nWhy other options fail:\n\u2022 All clusters suspend simultaneously after AUTO_SUSPEND timeout - This misunderstands multi-cluster behavior. Clusters scale down individually based on load, not as a group. When load decreases, excess clusters (above MIN) are released one at a time. Only when the warehouse is truly idle does AUTO_SUSPEND apply to the remaining minimum clusters.\n\u2022 Clusters scale down to 1 to conserve credits during low usage - The warehouse cannot go below MIN_CLUSTER_COUNT regardless of load. With MIN set to 2, scaling below 2 is impossible. The MIN setting guarantees minimum capacity for consistent baseline performance.\n\u2022 STANDARD policy maintains all 6 clusters until manual intervention - STANDARD policy is aggressive about scaling DOWN as well as up. It releases clusters as soon as they're not needed. ECONOMY policy is more conservative about scaling up (waits for queuing), but both scale down when load drops.\n\nKey exam tip: MIN_CLUSTER_COUNT is the floor - warehouse never runs fewer clusters while active. STANDARD scales aggressively both directions; ECONOMY is conservative scaling UP only.",
    "domain": "3",
    "topic": "Multi-cluster Warehouses",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "STANDARD policy maintains all 6 clusters until manual intervention",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2917
      },
      {
        "answerText": "Clusters scale down to MIN_CLUSTER_COUNT (2) but not below",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 2916
      },
      {
        "answerText": "Clusters scale down to 1 to conserve credits during low usage",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2915
      },
      {
        "answerText": "All clusters suspend simultaneously after AUTO_SUSPEND timeout",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2914
      }
    ]
  },
  {
    "id": 509,
    "questionText": "An organization needs to load data into Snowflake with minimal latency as files arrive in cloud storage. The data arrives continuously at approximately 50 small files per minute. Which ingestion approach provides the BEST balance of latency and cost efficiency?",
    "questionType": "single",
    "explanation": "Snowpipe is the optimal choice for continuous ingestion of small files arriving frequently. It uses serverless compute with event-driven triggers (S3 event notifications, Azure Event Grid, or GCS Pub/Sub) to detect and load files automatically within seconds of arrival. Snowpipe's billing model (per-file overhead + compute time based on file size) is ideal for many small files compared to keeping a warehouse running. Files are loaded as they arrive rather than waiting for scheduled batches.\n\nWhy other options fail:\n\u2022 Scheduled Task running COPY INTO every minute - This requires a warehouse to be running (costing credits) or resuming every minute (incurring startup overhead). Even with 1-minute scheduling, there's up to 60 seconds latency. For 50 files/minute, a warehouse sitting idle between loads wastes credits compared to Snowpipe's pay-per-file model.\n\u2022 Kafka connector with micro-batching enabled - The Snowflake Kafka Connector is designed for streaming data from Kafka topics, not for loading files from cloud storage. It requires Kafka infrastructure and is overkill for simple file-based ingestion. It adds unnecessary complexity for this use case.\n\u2022 Materialized view with REFRESH_ON_CREATE = TRUE - Materialized views are for query optimization (precomputing aggregations/joins), not data ingestion. REFRESH_ON_CREATE only affects initial population. Materialized views cannot ingest external files - they transform data already in Snowflake tables.\n\nKey exam tip: Snowpipe = serverless + event-driven + near-real-time file loading. Best for continuous small file ingestion. Snowpipe Streaming is for row-level streaming via API, not file loading.",
    "domain": "4",
    "topic": "Snowpipe",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Kafka connector with micro-batching enabled",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2920
      },
      {
        "answerText": "Scheduled Task running COPY INTO every minute",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2918
      },
      {
        "answerText": "Materialized view with REFRESH_ON_CREATE = TRUE",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2921
      },
      {
        "answerText": "Snowpipe with auto-ingest enabled via cloud event notifications",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 2919
      }
    ]
  },
  {
    "id": 510,
    "questionText": "A VARIANT column contains deeply nested JSON data. A query uses dot notation (data:customer.address.city) to extract a value, but returns NULL even though the data exists. What is the MOST likely cause?",
    "questionType": "single",
    "explanation": "VARIANT key access in Snowflake is case-sensitive by default. If the JSON key is 'Customer' but the query uses 'customer', it returns NULL with no error. This is a common gotcha because table/column names in Snowflake are case-insensitive (unless quoted), but JSON keys within VARIANT data preserve and require exact case matching. Solutions include: using bracket notation with exact case (data['Customer']['Address']['City']), using GET_PATH() with correct casing, or storing data with consistent lowercase keys during ingestion.\n\nWhy other options fail:\n\u2022 VARIANT columns cannot store more than 3 levels of nesting - VARIANT columns can store deeply nested structures with no practical nesting limit. Snowflake handles arbitrary nesting depth. The 16MB size limit applies to the entire VARIANT value, not nesting depth. Dot notation works at any depth with correct key names.\n\u2022 Dot notation requires explicit casting with ::STRING - While casting (::STRING, ::NUMBER) is needed to convert VARIANT results to specific types for some operations, it doesn't affect whether a value is found. Omitting the cast returns a VARIANT type, not NULL. Case mismatch returns NULL regardless of casting.\n\u2022 The VARIANT column exceeds the 16 MB compressed size limit - If a VARIANT value exceeded the limit, you'd get an error during insert, not NULL on read. Also, the 16MB limit is per-row VARIANT value, and exceeding it causes write failures, not silent NULL returns on queries.\n\nKey exam tip: VARIANT key access is CASE-SENSITIVE. data:Customer and data:customer are different paths. Always verify JSON key casing when debugging NULL results from VARIANT queries.",
    "domain": "5",
    "topic": "Data Types",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "The VARIANT column exceeds the 16 MB compressed size limit",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2925
      },
      {
        "answerText": "Dot notation requires explicit casting with ::STRING",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2924
      },
      {
        "answerText": "The JSON keys have different case than the dot notation path",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2923
      },
      {
        "answerText": "VARIANT columns cannot store more than 3 levels of nesting",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2922
      }
    ]
  },
  {
    "id": 511,
    "questionText": "A business intelligence team reports that their dashboard queries sometimes experience significant latency spikes during peak hours, while other times queries return instantly. The team uses a multi-cluster warehouse with SCALING_POLICY = ECONOMY. What change would MOST likely reduce latency variability?",
    "questionType": "single",
    "explanation": "ECONOMY scaling policy prioritizes cost savings over performance consistency by waiting for queries to queue before spinning up additional clusters. This means users experience delays during load spikes while Snowflake decides whether to add capacity. STANDARD policy spins up clusters immediately when queries begin queuing, providing more predictable performance at potentially higher cost. For BI dashboards where user experience depends on consistent response times, STANDARD eliminates the queuing delays that cause latency spikes.\n\nWhy other options fail:\n\u2022 Increase the warehouse size from Medium to Large - Warehouse size affects individual query execution speed (more compute = faster queries), but does NOT help with concurrency. If multiple queries are waiting because the warehouse can only run a limited number simultaneously, a larger warehouse still runs the same number of concurrent queries - just faster. The issue here is latency variability from queuing, not slow individual queries.\n\u2022 Set AUTO_SUSPEND to a higher value to maintain warm cache - A warmer cache helps with cold-start performance (first queries after resume), but doesn't address queuing delays from insufficient concurrency. The cache being warm means data is ready, but if queries are queuing for compute resources, cache warmth is irrelevant.\n\u2022 Enable Query Acceleration Service on the warehouse - QAS offloads portions of eligible queries to shared compute resources, helping specific query patterns (large scans with selective filters). It doesn't address multi-user concurrency issues. QAS helps individual query performance, not warehouse contention.\n\nKey exam tip: Latency variability during peak hours = concurrency/queuing issue. STANDARD scaling policy = consistent performance, higher cost. ECONOMY = cost savings, accepts some queuing.",
    "domain": "3",
    "topic": "Scaling Policies",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Enable Query Acceleration Service on the warehouse",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2929
      },
      {
        "answerText": "Increase the warehouse size from Medium to Large",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2926
      },
      {
        "answerText": "Change SCALING_POLICY to STANDARD",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2927
      },
      {
        "answerText": "Set AUTO_SUSPEND to a higher value to maintain warm cache",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2928
      }
    ]
  },
  {
    "id": 512,
    "questionText": "A data pipeline needs to process only the rows that changed since the last pipeline run. The source table receives approximately 10,000 inserts and 500 updates daily. Which combination of Snowflake objects provides the MOST efficient solution?",
    "questionType": "single",
    "explanation": "A Stream with APPEND_ONLY = FALSE (the default Standard stream type) combined with a scheduled Task provides efficient Change Data Capture (CDC). The stream tracks all DML changes (inserts, updates, deletes) since the last consumption by recording the change type in METADATA$ACTION and METADATA$ISUPDATE columns. When a Task's DML reads from the stream, the stream offset automatically advances, ensuring each change is processed exactly once. The Task can be scheduled at any interval to process accumulated changes.\n\nWhy other options fail:\n\u2022 Materialized view with automatic refresh tracking changes - Materialized views precompute and store query results, automatically refreshing when base data changes. However, they don't expose individual row changes - they just maintain a current snapshot. You cannot identify which specific rows were inserted/updated/deleted; you only see the refreshed result.\n\u2022 Dynamic table with TARGET_LAG for incremental processing - Dynamic tables automatically maintain transformed data with a specified lag. Like materialized views, they maintain current state rather than exposing change records. They're excellent for declarative pipelines but don't provide CDC row-level change tracking.\n\u2022 Time Travel query comparing current and historical snapshots - You could theoretically diff snapshots using Time Travel (AT/BEFORE clauses), but this is inefficient and complex. It requires comparing entire tables, doesn't track deletes well, and becomes costly for large tables. Streams are purpose-built for this and far more efficient.\n\nKey exam tip: Streams = CDC (change tracking). Standard stream (default) tracks INSERT/UPDATE/DELETE. APPEND_ONLY stream tracks only INSERTs (more efficient for insert-only tables). Tasks consume streams and auto-advance the offset.",
    "domain": "5",
    "topic": "Streams",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Dynamic table with TARGET_LAG for incremental processing",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2932
      },
      {
        "answerText": "Time Travel query comparing current and historical snapshots",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2933
      },
      {
        "answerText": "Stream (APPEND_ONLY = FALSE) combined with a scheduled Task",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2931
      },
      {
        "answerText": "Materialized view with automatic refresh tracking changes",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2930
      }
    ]
  },
  {
    "id": 513,
    "questionText": "A company is evaluating Snowflake editions for their data platform. They require: 90-day Time Travel, multi-cluster warehouses, and column-level security via masking policies. Which is the MINIMUM edition that meets ALL requirements?",
    "questionType": "single",
    "explanation": "Enterprise Edition is the minimum edition that provides all three required features. Multi-cluster warehouses require Enterprise Edition (Standard only supports single-cluster). Extended Time Travel up to 90 days requires Enterprise (Standard maxes at 1 day). Dynamic Data Masking policies for column-level security are also an Enterprise feature. All three requirements converge on Enterprise as the minimum.\n\nWhy other options fail:\n\u2022 Standard Edition with security add-on package - There is no 'security add-on package' in Snowflake. Standard Edition has hard limitations: 1-day max Time Travel, no multi-cluster warehouses, and no Dynamic Data Masking. These cannot be added via packages.\n\u2022 Business Critical Edition - While Business Critical includes all required features, it exceeds minimum requirements. It adds enhanced security (Tri-Secret Secure, Private Link, HIPAA/PCI compliance support) that aren't needed here. Enterprise is sufficient and more cost-effective.\n\u2022 Virtual Private Snowflake (VPS) - VPS is the highest tier, providing dedicated infrastructure and maximum isolation. It's significantly more expensive and includes capabilities far beyond these requirements. This violates the 'minimum edition' principle.\n\nKey exam tip: Know the edition feature matrix - Standard (basic), Enterprise (90-day TT, multi-cluster, masking, clustering), Business Critical (enhanced security, failover), VPS (dedicated infrastructure).",
    "domain": "1",
    "topic": "Editions",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Standard Edition with security add-on package",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2934
      },
      {
        "answerText": "Business Critical Edition",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2936
      },
      {
        "answerText": "Enterprise Edition",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2935
      },
      {
        "answerText": "Virtual Private Snowflake (VPS)",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2937
      }
    ]
  },
  {
    "id": 514,
    "questionText": "A permanent table with 90 days of Time Travel was dropped 95 days ago. The organization realized they need data from this table. What is the current state of this data?",
    "questionType": "single",
    "explanation": "After Time Travel expires, data enters Fail-safe for an additional 7 days. Fail-safe is a disaster recovery mechanism that Snowflake maintains for permanent tables - users cannot directly access it, only Snowflake Support can attempt recovery. The timeline: Day 0-90 = Time Travel (user-accessible), Day 90-97 = Fail-safe (Support only), Day 97+ = permanently deleted. At 95 days (90+5), the data is still in the 7-day Fail-safe window, so recovery may be possible by contacting Snowflake Support.\n\nWhy other options fail:\n\u2022 Data is permanently deleted - beyond both Time Travel and Fail-safe - This would be true at day 98+, but at 95 days, we're still within Fail-safe (days 90-97). The math: 95 days = 90 (Time Travel) + 5 (into Fail-safe), with 2 days remaining in Fail-safe.\n\u2022 Data is still in Time Travel - use UNDROP TABLE to recover - Time Travel expired at day 90. UNDROP TABLE only works within the Time Travel retention period. At 95 days, this window has closed and UNDROP will fail.\n\u2022 Data is archived in cold storage - restore using RESTORE TABLE command - There is no RESTORE TABLE command in Snowflake and no 'cold storage' archive concept. This describes functionality that doesn't exist.\n\nKey exam tip: Permanent tables: Time Travel (configurable, up to 90 days) + Fail-safe (always 7 days, Support-only). Transient/temporary tables have NO Fail-safe.",
    "domain": "6",
    "topic": "Fail-safe",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Data is archived in cold storage - restore using RESTORE TABLE command",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2941
      },
      {
        "answerText": "Data is permanently deleted - beyond both Time Travel and Fail-safe",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2938
      },
      {
        "answerText": "Data is still in Time Travel - use UNDROP TABLE to recover",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2940
      },
      {
        "answerText": "Data is in Fail-safe - contact Snowflake Support for potential recovery",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 2939
      }
    ]
  },
  {
    "id": 515,
    "questionText": "A JSON document stored in a VARIANT column contains an array of order items: {\"items\": [{\"sku\": \"A1\", \"qty\": 2}, {\"sku\": \"B2\", \"qty\": 1}]}. A report needs one row per item with the order_id from the parent row. Which query pattern correctly achieves this?",
    "questionType": "single",
    "explanation": "LATERAL FLATTEN is the correct approach for expanding arrays into rows while preserving parent row context. FLATTEN is a table function that takes an array or object and produces one row per element. The LATERAL keyword (or comma join syntax, which implies LATERAL) allows the FLATTEN function to reference columns from the preceding table. Key outputs: VALUE contains each array element, INDEX provides the array position, and you can access nested keys via VALUE:keyname.\n\nWhy other options fail:\n\u2022 SELECT order_id, PARSE_JSON(items) FROM orders - PARSE_JSON converts a string to VARIANT, not expand arrays. If 'items' is already VARIANT, this is unnecessary. Either way, it doesn't flatten the array into multiple rows - you'd still get one row per order with the array intact.\n\u2022 SELECT order_id, items[0]:sku, items[1]:sku FROM orders - This hardcodes array indices, only working for arrays with exactly 2 elements. It fails for variable-length arrays, produces NULLs for missing indices, and returns one row per order (not one row per item).\n\u2022 SELECT order_id, ARRAY_TO_STRING(data:items, ',') FROM orders - ARRAY_TO_STRING concatenates array elements into a single string. This produces one row per order with items as a comma-separated string, not individual rows per item. Also, it works on simple arrays, not arrays of objects.\n\nKey exam tip: FLATTEN + LATERAL = expand arrays/objects to rows. Remember the VALUE, INDEX, KEY, PATH, and THIS pseudo-columns that FLATTEN produces.",
    "domain": "5",
    "topic": "Semi-structured Data",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "SELECT o.order_id, f.value:sku, f.value:qty FROM orders o, LATERAL FLATTEN(input => o.data:items) f",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 2944
      },
      {
        "answerText": "SELECT order_id, ARRAY_TO_STRING(data:items, ',') FROM orders",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2945
      },
      {
        "answerText": "SELECT order_id, items[0]:sku, items[1]:sku FROM orders",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2943
      },
      {
        "answerText": "SELECT order_id, PARSE_JSON(items) FROM orders",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2942
      }
    ]
  },
  {
    "id": 516,
    "questionText": "A user runs an identical query twice. The first execution takes 45 seconds and scans 500GB. The second execution completes in 200 milliseconds with 0 bytes scanned. Between executions, no data changed and the warehouse remained running. Which caching layer provided the second result?",
    "questionType": "single",
    "explanation": "The Result Cache (Query Result Cache) stores complete query results at the Cloud Services layer for 24 hours. When an identical query is submitted (same SQL, same role, same warehouse context), Snowflake returns the cached result instantly without using any warehouse compute. The telltale signs: 0 bytes scanned, sub-second response, no warehouse credits consumed. Result cache is persisted in the Cloud Services layer, so it survives warehouse suspension.\n\nWhy other options fail:\n\u2022 Metadata cache - statistics provided instant row counts - The metadata cache stores table statistics (row counts, min/max values per micro-partition) and can answer specific queries like COUNT(*) instantly. However, the scenario describes a 500GB scan on first run - this wasn't a simple count query. Metadata cache doesn't store full query results.\n\u2022 Local disk cache - data cached on warehouse SSD - The warehouse cache (local disk/SSD cache) stores raw micro-partition data locally for faster re-reads. However, queries using this cache still show bytes scanned (from cache, not remote), still consume warehouse compute, and still take measurable time. It would NOT show 0 bytes scanned.\n\u2022 Micro-partition cache - frequently accessed partitions cached - This is essentially the same as local disk cache. There's no separate 'micro-partition cache' - it's the warehouse SSD cache. Same limitations apply: requires compute, shows bytes scanned.\n\nKey exam tip: 0 bytes scanned + instant results = Result Cache. The result cache is FREE, automatic, and persists 24 hours (or until underlying data changes).",
    "domain": "3",
    "topic": "Caching",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Micro-partition cache - frequently accessed partitions cached",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2949
      },
      {
        "answerText": "Local disk cache - data cached on warehouse SSD",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2948
      },
      {
        "answerText": "Metadata cache - statistics provided instant row counts",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2946
      },
      {
        "answerText": "Result cache - exact query results stored for 24 hours",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 2947
      }
    ]
  },
  {
    "id": 517,
    "questionText": "An administrator needs to create a new custom role called DATA_ANALYST and grant it SELECT on all current and future tables in a specific schema. Which system role should the administrator use to perform these actions following the principle of least privilege?",
    "questionType": "single",
    "explanation": "SECURITYADMIN is the minimum system role that can both create roles AND manage grants on any object. SECURITYADMIN inherits USERADMIN's ability to create/modify users and roles, but also has the global MANAGE GRANTS privilege allowing it to grant privileges on any object regardless of ownership. This makes it the appropriate role for creating a custom role and configuring both current and future grants, while following least privilege principles.\n\nWhy other options fail:\n\u2022 USERADMIN - specifically designed for user and role management - USERADMIN can create roles, but CANNOT grant arbitrary privileges on objects it doesn't own. Without MANAGE GRANTS, USERADMIN cannot configure SELECT on tables in a schema owned by another role. It's limited to user/role creation and management.\n\u2022 SYSADMIN - owns the schema so can manage its permissions - SYSADMIN can grant privileges on objects it owns, but may not own the specific schema mentioned. More importantly, SYSADMIN cannot create roles at all (that's USERADMIN/SECURITYADMIN territory). Using SYSADMIN for security administration isn't the intended pattern.\n\u2022 ACCOUNTADMIN - required for FUTURE GRANTS configuration - ACCOUNTADMIN CAN do everything needed, but violates least privilege. ACCOUNTADMIN should be reserved for account-level operations (resource monitors, integrations, etc.). FUTURE GRANTS don't require ACCOUNTADMIN - SECURITYADMIN with MANAGE GRANTS can configure them.\n\nKey exam tip: SECURITYADMIN = USERADMIN privileges + MANAGE GRANTS. Use SECURITYADMIN for role/grant administration, not ACCOUNTADMIN.",
    "domain": "2",
    "topic": "System Roles",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "ACCOUNTADMIN - required for FUTURE GRANTS configuration",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2953
      },
      {
        "answerText": "USERADMIN - specifically designed for user and role management",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2950
      },
      {
        "answerText": "SYSADMIN - owns the schema so can manage its permissions",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2952
      },
      {
        "answerText": "SECURITYADMIN - can create roles and manage all grants",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 2951
      }
    ]
  },
  {
    "id": 518,
    "questionText": "A 2TB table is frequently queried with WHERE clauses on both transaction_date (high cardinality) and region (low cardinality, 5 values). Queries filter on date ranges spanning 7-30 days. Which clustering key definition provides the BEST query performance?",
    "questionType": "single",
    "explanation": "Clustering keys should list lower cardinality columns first, followed by higher cardinality columns. This ordering maximizes partition pruning effectiveness. With region (5 values) first, a query filtering on a single region immediately eliminates approximately 80% of partitions. The transaction_date (higher cardinality) then provides fine-grained pruning within the region's partitions. This two-stage pruning is more effective than the reverse order.\n\nWhy other options fail:\n\u2022 CLUSTER BY (transaction_date) - Using only transaction_date ignores the region filter entirely. Queries must scan all regions even when filtering on a single region. For date ranges spanning 7-30 days, you'd scan date-clustered partitions across ALL 5 regions, missing significant pruning opportunity.\n\u2022 CLUSTER BY (transaction_date, region) - This order clusters primarily by date, with region as secondary. Date ranges (7-30 days) could span many partitions, and region filtering happens within those partitions rather than eliminating entire swaths upfront. Less effective than region-first approach.\n\u2022 CLUSTER BY (HASH(region), transaction_date) - HASH() destroys the natural ordering and distribution of region values. With only 5 regions, hashing is unnecessary and may actually worsen pruning. Hashing is useful for high-cardinality columns with skewed distribution, not low-cardinality columns like region.\n\nKey exam tip: Clustering key order matters: LOW cardinality first, then HIGH cardinality. Think of it as narrowing the search - eliminate the most partitions first with the broadest filter.",
    "domain": "3",
    "topic": "Clustering",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "CLUSTER BY (transaction_date)",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2954
      },
      {
        "answerText": "CLUSTER BY (HASH(region), transaction_date)",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2957
      },
      {
        "answerText": "CLUSTER BY (region, transaction_date)",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2955
      },
      {
        "answerText": "CLUSTER BY (transaction_date, region)",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2956
      }
    ]
  },
  {
    "id": 519,
    "questionText": "A development team clones a 500GB production database for testing. Immediately after cloning, they run ALTER TABLE on one table to add a column, then INSERT 1GB of test data. What is the approximate additional storage cost incurred by the clone?",
    "questionType": "single",
    "explanation": "Zero-copy cloning creates a new object that shares underlying micro-partitions with the source - no data is physically copied at clone time. Storage costs accrue ONLY when data diverges between source and clone. ALTER TABLE to add a column is a metadata-only operation that doesn't modify micro-partitions. The 1GB INSERT creates new micro-partitions only in the clone. Total additional storage is approximately 1GB.\n\nWhy other options fail:\n\u2022 501GB - full clone storage plus the new data - This assumes the clone copied all 500GB, which is incorrect. Zero-copy cloning means the clone initially consumes zero additional storage - it references the same micro-partitions as the source.\n\u2022 ~250GB - half the data is copied for the new table structure - ALTER TABLE ADD COLUMN is metadata-only. Existing micro-partitions aren't modified - new column has NULL values for existing rows without rewriting data.\n\u2022 0GB - clones share all storage including new data until production changes - New data inserted into the clone creates micro-partitions that belong ONLY to the clone. The 1GB insert is not shared with production.\n\nKey exam tip: Zero-copy cloning = instant, no initial storage cost. Storage diverges only when data is modified/added. Schema changes are metadata-only.",
    "domain": "6",
    "topic": "Cloning",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "~1GB - only the newly inserted data creates additional storage",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 2959
      },
      {
        "answerText": "501GB - full clone storage plus the new data",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2958
      },
      {
        "answerText": "0GB - clones share all storage including new data until production changes",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2961
      },
      {
        "answerText": "~250GB - half the data is copied for the new table structure",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2960
      }
    ]
  },
  {
    "id": 520,
    "questionText": "A Task is configured to run every 5 minutes and process data from a Stream. The stream currently shows 0 rows (no changes since last consumption). What happens when the Task's scheduled run occurs?",
    "questionType": "single",
    "explanation": "Without a WHEN clause, the task runs on schedule regardless of stream state - it executes the DML which processes 0 rows. Best practice is to add WHEN SYSTEM$STREAM_HAS_DATA('stream_name') which checks if the stream has unconsumed data. When false, the task skips execution, saving compute costs.\n\nWhy other options fail:\n\u2022 Task fails with an error - streams cannot be empty - Streams can be empty. An empty stream means no changes since last consumption. Querying returns 0 rows without error.\n\u2022 Task automatically suspends until stream has data - Tasks don't auto-suspend based on stream state. Without WHEN clause, it runs every scheduled interval. With WHEN clause, it checks condition but doesn't suspend.\n\u2022 Task reprocesses the last batch of data from stream history - Streams don't maintain history of consumed data. Once consumed, the offset advances and that data is gone. There's no replay mechanism.\n\nKey exam tip: WHEN SYSTEM$STREAM_HAS_DATA('stream_name') is the idiomatic pattern for stream-triggered tasks. It prevents empty runs and saves compute costs.",
    "domain": "5",
    "topic": "Tasks",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Task runs but processes 0 rows; WHEN clause can skip empty runs",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 2963
      },
      {
        "answerText": "Task reprocesses the last batch of data from stream history",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2965
      },
      {
        "answerText": "Task fails with an error - streams cannot be empty",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2962
      },
      {
        "answerText": "Task automatically suspends until stream has data",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2964
      }
    ]
  },
  {
    "id": 521,
    "questionText": "A query consistently runs for 3 minutes on an X-Small warehouse. The query profile shows it's scanning 200GB with good partition pruning. The team wants to reduce execution time to under 1 minute. Which approach is MOST likely to achieve this goal?",
    "questionType": "single",
    "explanation": "Warehouse size directly impacts query parallelism and compute power. Each size increase (X-Small to Small to Medium to Large) doubles compute resources. Moving from X-Small to Medium (2 size increases = 4x compute) should theoretically reduce a 3-minute query to approximately 45 seconds. Since the query shows good pruning, it's compute-bound, making larger warehouse the solution.\n\nWhy other options fail:\n\u2022 Enable multi-cluster warehousing with 4 clusters - Multi-cluster helps with concurrent query throughput (many users running queries simultaneously), NOT individual query speed. A single query uses only one cluster. Adding clusters doesn't make your query faster.\n\u2022 Enable Query Acceleration Service with max scale factor - QAS offloads portions of eligible queries to shared compute pools. It helps specific patterns (large scans with selective filters) but isn't guaranteed to help all queries. Also, with 'good partition pruning' already achieved, QAS may have limited benefit.\n\u2022 Configure automatic clustering on the scanned table - Clustering improves partition pruning. The question states pruning is already 'good,' so clustering won't help much. Clustering addresses data organization, not raw compute power for well-pruned queries.\n\nKey exam tip: For single-query performance: increase warehouse SIZE. For concurrent query handling: add CLUSTERS. Size = speed, Clusters = throughput.",
    "domain": "3",
    "topic": "Warehouse Sizing",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Configure automatic clustering on the scanned table",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2969
      },
      {
        "answerText": "Increase warehouse size from X-Small to Medium or Large",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 2967
      },
      {
        "answerText": "Enable Query Acceleration Service with max scale factor",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2968
      },
      {
        "answerText": "Enable multi-cluster warehousing with 4 clusters",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2966
      }
    ]
  },
  {
    "id": 522,
    "questionText": "A data provider wants to share a subset of their customer data with a partner organization that also uses Snowflake. The shared data must be filtered so partners only see their own customers. Which approach enables this while allowing the provider to maintain a SINGLE copy of the data?",
    "questionType": "single",
    "explanation": "Secure Data Sharing with a Secure View containing row-level filtering is the solution. The provider creates a secure view with WHERE clause using CURRENT_ACCOUNT() to filter rows based on the consumer's account identifier. This view is added to a share and granted to partners. Each partner sees only their own data while the provider maintains a SINGLE physical copy. No data movement, no copying, real-time access to current data.\n\nWhy other options fail:\n\u2022 Data Replication with consumer-specific databases - Replication creates actual copies of data in other accounts/regions. This violates the 'single copy' requirement. Each consumer would have their own copy, increasing storage costs and requiring sync management.\n\u2022 Snowflake Marketplace listing with access controls - Marketplace is for publishing data products broadly (free or paid). It's not designed for filtered, partner-specific data sharing where each partner sees different rows. Marketplace listings show the same data to all consumers.\n\u2022 Data Clean Room with approved query templates - Data Clean Rooms enable privacy-preserving analytics where parties can run approved queries against combined data without exposing raw records. The use case here is straightforward filtered sharing, not joint analytics. Clean rooms add unnecessary complexity.\n\nKey exam tip: Secure View + CURRENT_ACCOUNT() = row-level filtering for shares. The view is 'secure' to prevent consumers from reverse-engineering the filter logic through query plans.",
    "domain": "6",
    "topic": "Data Sharing",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Snowflake Marketplace listing with access controls",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2972
      },
      {
        "answerText": "Data Clean Room with approved query templates",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2973
      },
      {
        "answerText": "Data Replication with consumer-specific databases",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2970
      },
      {
        "answerText": "Secure Data Sharing with a Secure View containing row-level filtering",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 2971
      }
    ]
  },
  {
    "id": 523,
    "questionText": "What is the purpose of the PUT command in Snowflake?",
    "questionType": "single",
    "explanation": "The PUT command uploads files from a local file system to a Snowflake internal stage (user stage @~, table stage @%tablename, or named internal stage @stage_name). It compresses files automatically by default and can only be executed from SnowSQL or Snowflake drivers/connectors, not from the web UI.\n\nWhy other options fail:\n\u2022 Insert data into a table: INSERT or COPY INTO commands insert data into tables, not PUT. PUT only stages files.\n\u2022 Create a new database: CREATE DATABASE creates databases. PUT has nothing to do with database creation.\n\u2022 Grant privileges to a role: GRANT command manages privileges. PUT is solely for file staging.\n\nKey exam tip: PUT is for internal stages only. For external stages (S3, Azure, GCS), files are uploaded using native cloud tools, not PUT.",
    "domain": "4",
    "topic": "Data Loading",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Grant privileges to a role",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2977
      },
      {
        "answerText": "Upload files to an internal stage",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 2975
      },
      {
        "answerText": "Insert data into a table",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2974
      },
      {
        "answerText": "Create a new database",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2976
      }
    ]
  },
  {
    "id": 524,
    "questionText": "Which Snowflake component stores table data in compressed, columnar format?",
    "questionType": "single",
    "explanation": "Micro-partitions are Snowflake's fundamental storage units, containing 50-500MB of uncompressed data each. Data is stored in compressed, columnar format with rich metadata (min/max values, distinct counts, NULL counts) enabling efficient pruning. Micro-partitions are immutable - updates create new partitions rather than modifying existing ones.\n\nWhy other options fail:\n\u2022 Data blocks: This is a generic term used in traditional databases but not Snowflake's specific terminology.\n\u2022 Segments: Segments are used in other systems like Oracle or SQL Server, not Snowflake's architecture.\n\u2022 Extents: Extents are allocation units in traditional databases (Oracle, SQL Server) but not a Snowflake concept.\n\nKey exam tip: Remember the 50-500MB size range and that micro-partitions are immutable. Snowflake automatically handles partitioning - there's no manual partition management required.",
    "domain": "1",
    "topic": "Storage",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Extents",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2981
      },
      {
        "answerText": "Segments",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2980
      },
      {
        "answerText": "Data blocks",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2978
      },
      {
        "answerText": "Micro-partitions",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 2979
      }
    ]
  },
  {
    "id": 525,
    "questionText": "Which role is automatically granted to every user in Snowflake?",
    "questionType": "single",
    "explanation": "PUBLIC is a special system role automatically granted to every user in Snowflake and cannot be revoked. Any privileges granted to PUBLIC become available to all users in the account, making it useful for sharing common objects. Objects granted to PUBLIC are accessible by everyone.\n\nWhy other options fail:\n\u2022 USER: There is no system role called USER in Snowflake. Users are principals, not roles.\n\u2022 DEFAULT: Not a Snowflake role. DEFAULT_ROLE is a user property specifying which role activates at login, not a role itself.\n\u2022 BASIC: This is not a Snowflake role. There's no role named BASIC in Snowflake's role hierarchy.\n\nKey exam tip: Be careful not to confuse PUBLIC (the role granted to all users) with DEFAULT_ROLE (the user property that sets the initial session role). Also remember the role hierarchy: ACCOUNTADMIN > SECURITYADMIN/SYSADMIN > PUBLIC.",
    "domain": "2",
    "topic": "Roles",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "BASIC",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2985
      },
      {
        "answerText": "DEFAULT",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2984
      },
      {
        "answerText": "PUBLIC",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2983
      },
      {
        "answerText": "USER",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2982
      }
    ]
  },
  {
    "id": 526,
    "questionText": "Which type of view in Snowflake pre-computes and stores results for faster query performance?",
    "questionType": "single",
    "explanation": "Materialized views physically store pre-computed query results and automatically maintain synchronization with base table data through background refresh. They dramatically improve query performance for complex aggregations or joins by avoiding repeated computation. Materialized views incur storage costs and serverless compute costs for maintenance.\n\nWhy other options fail:\n\u2022 Standard view: Standard views are simply stored query definitions - they don't store results. Each query re-executes the view definition.\n\u2022 Secure view: Secure views hide the view definition and optimize for security, not performance. They don't pre-compute or store results.\n\u2022 Dynamic view: This is not a Snowflake view type. Dynamic tables exist but serve a different purpose (pipeline transformation).\n\nKey exam tip: Materialized views have restrictions - they cannot include UDFs, JOINs on other views, or non-deterministic functions. They require Enterprise Edition or higher and consume both storage and serverless compute credits.",
    "domain": "3",
    "topic": "Views",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Materialized view",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 2987
      },
      {
        "answerText": "Secure view",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2988
      },
      {
        "answerText": "Dynamic view",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2989
      },
      {
        "answerText": "Standard view",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2986
      }
    ]
  },
  {
    "id": 527,
    "questionText": "What is the maximum Time Travel retention period in Snowflake Enterprise Edition?",
    "questionType": "single",
    "explanation": "Enterprise Edition (and higher) supports up to 90 days of Time Travel retention for permanent tables via the DATA_RETENTION_TIME_IN_DAYS parameter. This enables querying historical data, cloning from past states, and restoring dropped objects within the retention window.\n\nWhy other options fail:\n\u2022 7 days: This is a common default value in many systems but not Snowflake's maximum. The default in Enterprise is 1 day, maximum is 90.\n\u2022 30 days: While this might seem reasonable, Snowflake Enterprise actually supports up to 90 days.\n\u2022 365 days: Snowflake does not support a full year of Time Travel. 90 days is the maximum in any edition.\n\nKey exam tip: Standard Edition maximum is 1 day. Enterprise Edition maximum is 90 days. Transient and temporary tables have a maximum of 1 day regardless of edition. Higher retention = higher storage costs due to Fail-safe overlap.",
    "domain": "6",
    "topic": "Time Travel",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "365 days",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2993
      },
      {
        "answerText": "90 days",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 2992
      },
      {
        "answerText": "7 days",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2990
      },
      {
        "answerText": "30 days",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2991
      }
    ]
  },
  {
    "id": 528,
    "questionText": "Which Snowflake feature prevents users from seeing the underlying query definition of a view?",
    "questionType": "single",
    "explanation": "Secure views hide the view definition (DDL) and query execution plan from users who don't own the view. This protects intellectual property, sensitive business logic, and prevents users from reverse-engineering data access patterns. Created using CREATE SECURE VIEW or ALTER VIEW ... SET SECURE.\n\nWhy other options fail:\n\u2022 Private views: There is no view type called 'private view' in Snowflake.\n\u2022 Encrypted views: Not a Snowflake concept. All data is encrypted by default, but there's no special 'encrypted view' type.\n\u2022 Hidden views: This is not Snowflake terminology. Views can be secure, but not 'hidden' as a type.\n\nKey exam tip: Secure views may have slightly lower performance because the optimizer cannot share certain optimizations with the user. They're required for secure data sharing. SHOW VIEWS displays 'Y' in the is_secure column for secure views.",
    "domain": "2",
    "topic": "Secure Views",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Private views",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 2994
      },
      {
        "answerText": "Secure views",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 2995
      },
      {
        "answerText": "Encrypted views",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 2996
      },
      {
        "answerText": "Hidden views",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 2997
      }
    ]
  },
  {
    "id": 529,
    "questionText": "What is the purpose of a Resource Monitor in Snowflake?",
    "questionType": "single",
    "explanation": "Resource monitors track credit consumption and trigger actions when thresholds are reached. They can be assigned at the account level or to specific warehouses. Actions include NOTIFY (send alerts), SUSPEND (stop warehouse after current queries), and SUSPEND_IMMEDIATE (stop warehouse and cancel running queries).\n\nWhy other options fail:\n\u2022 Monitor query performance: Query performance is tracked via Query History, Query Profile, and ACCOUNT_USAGE views, not resource monitors.\n\u2022 Monitor storage usage: Storage is tracked separately through ACCOUNT_USAGE.STORAGE_USAGE or ORGANIZATION_USAGE views, not resource monitors.\n\u2022 Track user logins: Login history is tracked via ACCOUNT_USAGE.LOGIN_HISTORY, not resource monitors.\n\nKey exam tip: Only ACCOUNTADMIN can create resource monitors. Monitors can have multiple thresholds (e.g., notify at 75%, suspend at 100%). Credit quotas reset based on the schedule (daily, weekly, monthly, yearly, or never).",
    "domain": "3",
    "topic": "Resource Monitors",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Monitor storage usage",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3000
      },
      {
        "answerText": "Monitor query performance",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 2998
      },
      {
        "answerText": "Track and control credit consumption",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 2999
      },
      {
        "answerText": "Track user logins",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3001
      }
    ]
  },
  {
    "id": 530,
    "questionText": "Which cloud providers does Snowflake support for deployment?",
    "questionType": "multi",
    "explanation": "Snowflake runs natively on three major cloud platforms: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). Customers select their cloud provider and specific region during account setup. Each deployment is isolated to a single cloud/region, but cross-cloud data sharing is possible.\n\nWhy other options fail:\n\u2022 Oracle Cloud: Snowflake does not support Oracle Cloud Infrastructure (OCI) as a deployment platform. Only AWS, Azure, and GCP are supported.\n\nKey exam tip: Snowflake accounts are region-specific - data resides in your chosen cloud/region. Replication can copy data across regions and clouds. The Snowflake Marketplace and data sharing work across all supported clouds.",
    "domain": "1",
    "topic": "Cloud Providers",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Google Cloud Platform (GCP)",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3004
      },
      {
        "answerText": "Microsoft Azure",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3003
      },
      {
        "answerText": "Oracle Cloud",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3005
      },
      {
        "answerText": "Amazon Web Services (AWS)",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3002
      }
    ]
  },
  {
    "id": 531,
    "questionText": "What does the UNDROP command do in Snowflake?",
    "questionType": "single",
    "explanation": "UNDROP restores dropped objects (tables, schemas, databases) that are still within their Time Travel retention period. The syntax is UNDROP TABLE/SCHEMA/DATABASE object_name. The restored object returns to its original state at the moment it was dropped, including all data.\n\nWhy other options fail:\n\u2022 Permanently deletes an object: DROP permanently removes objects (after Time Travel expires). UNDROP does the opposite - it restores.\n\u2022 Removes all data from a table: TRUNCATE or DELETE removes data. UNDROP restores previously dropped objects.\n\u2022 Reverts the last transaction: ROLLBACK reverts transactions. UNDROP specifically recovers dropped objects, not transactional changes.\n\nKey exam tip: If you drop and recreate an object with the same name, you must rename the new object first before using UNDROP. Objects in Fail-safe cannot be recovered via UNDROP - only Snowflake support can recover Fail-safe data.",
    "domain": "6",
    "topic": "Time Travel",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Reverts the last transaction",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3009
      },
      {
        "answerText": "Permanently deletes an object",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3006
      },
      {
        "answerText": "Removes all data from a table",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3008
      },
      {
        "answerText": "Restores a dropped object from Time Travel",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3007
      }
    ]
  },
  {
    "id": 532,
    "questionText": "Which stage type is automatically created for each user in Snowflake?",
    "questionType": "single",
    "explanation": "User stages are automatically created for each Snowflake user and referenced with @~ (tilde notation). They provide personal, isolated storage for staging files. Only the owning user can access their user stage - it cannot be shared or granted to others.\n\nWhy other options fail:\n\u2022 Named stage: Named stages must be explicitly created with CREATE STAGE. They're not automatic and are referenced by @stage_name.\n\u2022 Table stage: Table stages are automatically created per table (not per user) and referenced with @%table_name. Used for loading data into specific tables.\n\u2022 External stage: External stages reference cloud storage (S3, Azure Blob, GCS) and must be explicitly created with storage credentials.\n\nKey exam tip: Remember the three internal stage types and their notation: User (@~), Table (@%tablename), Named (@stagename). User and table stages cannot be altered or dropped - only named stages can be managed with DDL commands.",
    "domain": "4",
    "topic": "Stages",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "External stage",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3013
      },
      {
        "answerText": "User stage",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3011
      },
      {
        "answerText": "Named stage",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3010
      },
      {
        "answerText": "Table stage",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3012
      }
    ]
  },
  {
    "id": 533,
    "questionText": "What is the purpose of Dynamic Data Masking in Snowflake?",
    "questionType": "single",
    "explanation": "Dynamic Data Masking applies masking policies to columns at query time, showing different values based on the querying user's role. The underlying data remains unchanged in storage - masking is applied dynamically during retrieval. This protects PII, financial data, and other sensitive information while allowing authorized users full access.\n\nWhy other options fail:\n\u2022 Encrypt data at rest: Snowflake automatically encrypts all data at rest with AES-256. This is separate from masking, which controls what users see.\n\u2022 Compress data for storage efficiency: Compression is automatic in micro-partitions and unrelated to data masking functionality.\n\u2022 Anonymize data permanently: Masking is dynamic and reversible based on role. Permanent anonymization would require transforming the actual stored data.\n\nKey exam tip: Masking policies use CASE statements to return different values based on CURRENT_ROLE() or IS_ROLE_IN_SESSION(). Policies are created with CREATE MASKING POLICY and applied with ALTER TABLE/COLUMN. This is an Enterprise Edition feature.",
    "domain": "2",
    "topic": "Data Masking",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Encrypt data at rest",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3014
      },
      {
        "answerText": "Hide sensitive data based on user role",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3015
      },
      {
        "answerText": "Anonymize data permanently",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3017
      },
      {
        "answerText": "Compress data for storage efficiency",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3016
      }
    ]
  },
  {
    "id": 534,
    "questionText": "Which SQL command creates a User-Defined Function (UDF) in Snowflake?",
    "questionType": "single",
    "explanation": "CREATE FUNCTION is the SQL command to create User-Defined Functions (UDFs) in Snowflake. UDFs can be written in SQL, JavaScript, Python, Java, or Scala and extend Snowflake's capabilities with custom business logic. They return a value and can be scalar (single value) or tabular (UDTF - returns rows).\n\nWhy other options fail:\n\u2022 CREATE UDF: This syntax does not exist in Snowflake. The correct command is CREATE FUNCTION.\n\u2022 DEFINE FUNCTION: Not a valid Snowflake command. Some languages use DEFINE, but Snowflake uses CREATE.\n\u2022 CREATE PROCEDURE: This creates stored procedures, which execute statements and may have side effects. Procedures are different from functions - they use CALL to execute, not SELECT.\n\nKey exam tip: Functions return values and are used in SELECT statements. Procedures perform actions and are invoked with CALL. Python UDFs require Snowpark. JavaScript UDFs have been available longest and have the broadest support.",
    "domain": "5",
    "topic": "UDFs",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "CREATE FUNCTION",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3019
      },
      {
        "answerText": "DEFINE FUNCTION",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3020
      },
      {
        "answerText": "CREATE UDF",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3018
      },
      {
        "answerText": "CREATE PROCEDURE",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3021
      }
    ]
  },
  {
    "id": 535,
    "questionText": "What happens to the warehouse local disk cache when the warehouse is suspended?",
    "questionType": "single",
    "explanation": "When a warehouse suspends, the local disk cache (also called warehouse cache or SSD cache) is completely cleared because the underlying compute resources and their attached storage are released. Upon resumption, the cache starts empty and must be rebuilt through subsequent queries - this is called 'warming up' the cache.\n\nWhy other options fail:\n\u2022 Cache is preserved for 24 hours: Cache cannot be preserved because the compute nodes with local SSDs are deallocated on suspend.\n\u2022 Cache is moved to cloud storage: Moving cache to cloud storage would defeat its purpose (fast local access). Cache is simply discarded.\n\u2022 Cache is shared with other warehouses: Each warehouse has its own isolated cache. Warehouses never share cache, even when running.\n\nKey exam tip: Snowflake has three cache layers: Query Result Cache (24 hours, free, in Cloud Services), Metadata Cache (Cloud Services), and Local Disk/Warehouse Cache (lost on suspend). Consider the tradeoff between cost savings from suspension vs. cache rebuild time.",
    "domain": "3",
    "topic": "Caching",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Cache is moved to cloud storage",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3024
      },
      {
        "answerText": "Cache is cleared",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3023
      },
      {
        "answerText": "Cache is preserved for 24 hours",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3022
      },
      {
        "answerText": "Cache is shared with other warehouses",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3025
      }
    ]
  },
  {
    "id": 536,
    "questionText": "Which Snowflake feature allows running Python code directly in the data warehouse?",
    "questionType": "single",
    "explanation": "Snowpark is Snowflake's developer framework that enables running Python, Java, and Scala code directly within Snowflake's compute environment. It provides a DataFrame API similar to Apache Spark, allowing developers to build data pipelines that push computation to the warehouse instead of moving data to client applications.\n\nWhy other options fail:\n\u2022 Python UDFs only: While Python UDFs allow Python code, Snowpark is more comprehensive - it includes DataFrame API, stored procedures, and the ability to run entire applications, not just functions.\n\u2022 External Functions: External functions call APIs hosted outside Snowflake (AWS Lambda, Azure Functions, etc.). Code runs externally, not within Snowflake.\n\u2022 Snowpipe: Snowpipe is for continuous data loading from stages, not for running code. It automates COPY INTO operations.\n\nKey exam tip: Snowpark pushes code execution to Snowflake, keeping data in place. This is different from traditional approaches where data moves to the application. Snowpark requires specific client libraries and supports ML model training/inference directly in Snowflake.",
    "domain": "1",
    "topic": "Snowpark",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Snowpipe",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3029
      },
      {
        "answerText": "Python UDFs only",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3026
      },
      {
        "answerText": "Snowpark",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3027
      },
      {
        "answerText": "External Functions",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3028
      }
    ]
  },
  {
    "id": 537,
    "questionText": "What is the purpose of the SYSADMIN role in Snowflake?",
    "questionType": "single",
    "explanation": "SYSADMIN (System Administrator) is the system-defined role responsible for creating and managing databases, schemas, warehouses, and other database objects. Snowflake recommends creating custom roles that grant to SYSADMIN, forming a hierarchy where SYSADMIN can manage all objects in the account.\n\nWhy other options fail:\n\u2022 Manage users and roles only: SECURITYADMIN and USERADMIN manage users and roles. SYSADMIN focuses on database objects, not security principals.\n\u2022 Manage billing and payments: ORGADMIN handles organization-level billing. ACCOUNTADMIN can view usage, but billing management is typically through the web portal.\n\u2022 Monitor system performance only: Monitoring is available to multiple roles. SYSADMIN's primary purpose is object management, not just monitoring.\n\nKey exam tip: The role hierarchy is ACCOUNTADMIN > SYSADMIN/SECURITYADMIN > custom roles > PUBLIC. Best practice: Use SYSADMIN for object creation, SECURITYADMIN for access control, and reserve ACCOUNTADMIN for critical administrative tasks only.",
    "domain": "2",
    "topic": "System Roles",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Create and manage databases and warehouses",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3031
      },
      {
        "answerText": "Manage users and roles only",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3030
      },
      {
        "answerText": "Manage billing and payments",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3032
      },
      {
        "answerText": "Monitor system performance only",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3033
      }
    ]
  },
  {
    "id": 538,
    "questionText": "Which file format is NOT natively supported by Snowflake for loading data?",
    "questionType": "single",
    "explanation": "Snowflake natively supports loading these file formats: CSV, JSON, Avro, ORC, Parquet, and XML. Excel files (.xlsx, .xls) are NOT directly supported and must be converted to CSV or another supported format before loading into Snowflake.\n\nWhy other options fail:\n\u2022 CSV: CSV is natively supported - it's the most common format for bulk loading with COPY INTO.\n\u2022 Parquet: Parquet is natively supported - it's a popular columnar format often used with data lakes and big data tools.\n\u2022 JSON: JSON is natively supported - Snowflake has extensive semi-structured data capabilities for JSON data stored in VARIANT columns.\n\nKey exam tip: Remember the six supported formats: CSV, JSON, Avro, ORC, Parquet, XML. File format objects (CREATE FILE FORMAT) define parsing options. For Parquet and ORC, Snowflake can query data directly in stages using schema inference.",
    "domain": "4",
    "topic": "File Formats",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "CSV",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3034
      },
      {
        "answerText": "Parquet",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3035
      },
      {
        "answerText": "Excel (.xlsx)",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3036
      },
      {
        "answerText": "JSON",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3037
      }
    ]
  },
  {
    "id": 539,
    "questionText": "What does the OBJECT_CONSTRUCT function do in Snowflake?",
    "questionType": "single",
    "explanation": "OBJECT_CONSTRUCT creates a JSON/VARIANT object from key-value pairs. Syntax: OBJECT_CONSTRUCT('key1', value1, 'key2', value2, ...). It's invaluable for transforming relational data into semi-structured format, building nested JSON structures, or preparing data for export.\n\nWhy other options fail:\n\u2022 Creates a table from JSON: COPY INTO or INSERT creates tables/rows from JSON. OBJECT_CONSTRUCT builds JSON objects, not tables.\n\u2022 Parses JSON into columns: FLATTEN or dot notation (data:key) parses JSON into columns. OBJECT_CONSTRUCT does the reverse - it creates JSON.\n\u2022 Validates JSON syntax: TRY_PARSE_JSON or CHECK_JSON validate JSON. OBJECT_CONSTRUCT constructs valid JSON, it doesn't validate existing JSON.\n\nKey exam tip: OBJECT_CONSTRUCT automatically excludes NULL values by default. Use OBJECT_CONSTRUCT_KEEP_NULL to include NULLs. Related functions: ARRAY_CONSTRUCT (builds arrays), OBJECT_INSERT (adds keys), OBJECT_DELETE (removes keys).",
    "domain": "5",
    "topic": "Semi-structured Functions",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Parses JSON into columns",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3040
      },
      {
        "answerText": "Builds a JSON object from key-value pairs",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3039
      },
      {
        "answerText": "Creates a table from JSON",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3038
      },
      {
        "answerText": "Validates JSON syntax",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3041
      }
    ]
  },
  {
    "id": 540,
    "questionText": "Which Snowflake edition includes Tri-Secret Secure encryption?",
    "questionType": "single",
    "explanation": "Tri-Secret Secure is available exclusively in Business Critical Edition and higher (including VPS). It combines a customer-managed key (via cloud KMS) with Snowflake's key to create a composite master key. This gives customers control over their encryption - revoking the customer key renders data inaccessible, even to Snowflake.\n\nWhy other options fail:\n\u2022 Standard Edition: Standard has basic encryption (Snowflake-managed keys only). No customer-managed key support.\n\u2022 Enterprise Edition: Enterprise adds features like multi-cluster warehouses and 90-day Time Travel, but not Tri-Secret Secure.\n\u2022 All editions: Tri-Secret Secure requires Business Critical or higher due to the enhanced security infrastructure needed.\n\nKey exam tip: The 'three secrets' are: (1) Snowflake's root key, (2) Snowflake's account key, (3) Customer's key from cloud KMS (AWS KMS, Azure Key Vault, or GCP Cloud KMS). Business Critical also includes HIPAA and PCI-DSS compliance support.",
    "domain": "2",
    "topic": "Encryption",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Enterprise Edition",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3043
      },
      {
        "answerText": "Business Critical Edition",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3044
      },
      {
        "answerText": "All editions",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3045
      },
      {
        "answerText": "Standard Edition",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3042
      }
    ]
  },
  {
    "id": 541,
    "questionText": "What is the purpose of a Share object in Snowflake?",
    "questionType": "single",
    "explanation": "A Share is a Snowflake object that encapsulates database objects (databases, schemas, tables, secure views, UDFs) to be shared with other Snowflake accounts. The Share defines what data consumers can access. Data is not copied - consumers query the provider's data in place, seeing live, current data.\n\nWhy other options fail:\n\u2022 Copy data to another account: Shares provide live access without copying data. This is a key differentiator - no data duplication, no ETL, no sync delays.\n\u2022 Create a backup of the database: Backups use Time Travel, Fail-safe, or replication. Shares are for external data access, not backup.\n\u2022 Share compute resources between accounts: Each account uses its own warehouses. Shares only share data access, never compute resources.\n\nKey exam tip: Providers CREATE SHARE and GRANT privileges on objects to the share, then add consumer accounts. Consumers create a database FROM SHARE. Secure views are commonly used to control row/column access within shares. Data Sharing is included in all editions at no additional cost.",
    "domain": "6",
    "topic": "Data Sharing",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Create a backup of the database",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3048
      },
      {
        "answerText": "Copy data to another account",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3046
      },
      {
        "answerText": "Define objects accessible to other accounts",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3047
      },
      {
        "answerText": "Share compute resources between accounts",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3049
      }
    ]
  },
  {
    "id": 542,
    "questionText": "Which statement about Snowflake's data storage is TRUE?",
    "questionType": "single",
    "explanation": "Snowflake automatically compresses all data using columnar compression techniques optimized for each column's data type and patterns. Users cannot choose or configure compression algorithms - Snowflake's storage layer intelligently determines the optimal compression method per column. This typically achieves 4:1 to 10:1 compression ratios.\n\nWhy other options fail:\n\u2022 Users must manually compress data before loading: Snowflake handles compression automatically during data loading. Pre-compressing data is unnecessary and counterproductive.\n\u2022 Compression is optional and disabled by default: Compression is always on and cannot be disabled. It's fundamental to Snowflake's storage efficiency.\n\u2022 Only certain data types can be compressed: All data types are compressed. Snowflake applies appropriate compression algorithms to every column regardless of type.\n\nKey exam tip: Storage billing is based on compressed data size, not uncompressed. Snowflake uses different compression algorithms for different data patterns (run-length encoding, dictionary encoding, etc.). You're billed for actual compressed bytes stored.",
    "domain": "1",
    "topic": "Storage",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Users must manually compress data before loading",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3050
      },
      {
        "answerText": "Compression is optional and disabled by default",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3052
      },
      {
        "answerText": "Data is automatically compressed using columnar compression",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3051
      },
      {
        "answerText": "Only certain data types can be compressed",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3053
      }
    ]
  },
  {
    "id": 543,
    "questionText": "What is the AUTO_SUSPEND parameter used for in warehouse configuration?",
    "questionType": "single",
    "explanation": "AUTO_SUSPEND specifies the number of seconds of inactivity after which a warehouse automatically suspends, stopping credit consumption. The minimum value is 60 seconds (or 0 to disable auto-suspend). When a warehouse suspends, the compute cluster is released and no credits are consumed until it resumes.\n\nWhy other options fail:\n\u2022 Automatic scaling of clusters - This is controlled by the SCALING_POLICY and MIN_CLUSTER_COUNT/MAX_CLUSTER_COUNT parameters for multi-cluster warehouses, not AUTO_SUSPEND.\n\u2022 Maximum query execution time - Query timeout is controlled by the STATEMENT_TIMEOUT_IN_SECONDS parameter, which is separate from warehouse suspension.\n\u2022 Session timeout setting - Session timeouts are managed at the session or account level, not through warehouse configuration.\n\nKey exam tip: AUTO_SUSPEND works with AUTO_RESUME to enable on-demand compute - warehouses suspend when idle and resume automatically when queries arrive.",
    "domain": "3",
    "topic": "Warehouse Configuration",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Maximum query execution time",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3056
      },
      {
        "answerText": "Session timeout setting",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3057
      },
      {
        "answerText": "Time before warehouse suspends due to inactivity",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3055
      },
      {
        "answerText": "Automatic scaling of clusters",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3054
      }
    ]
  },
  {
    "id": 544,
    "questionText": "Which type of stored procedure runs with the privileges of the owner rather than the caller?",
    "questionType": "single",
    "explanation": "Owner's rights stored procedures execute using the privileges of the user who created (owns) the procedure, regardless of who calls it. This allows the procedure to access objects the caller may not have direct permissions on, making it useful for controlled data access patterns. Owner's rights is the default behavior in Snowflake.\n\nWhy other options fail:\n\u2022 Caller's rights procedure - This is the opposite; caller's rights procedures run with the privileges of the user executing them, requiring the caller to have necessary permissions on all accessed objects.\n\u2022 Admin rights procedure - This is not a valid Snowflake procedure type. There is no 'admin rights' designation for stored procedures.\n\u2022 Elevated rights procedure - This is not a Snowflake concept. Snowflake uses owner's rights and caller's rights, not 'elevated rights.'\n\nKey exam tip: Use EXECUTE AS CALLER to create a caller's rights procedure. Owner's rights is default and useful for encapsulating privileged operations.",
    "domain": "5",
    "topic": "Stored Procedures",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Elevated rights procedure",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3061
      },
      {
        "answerText": "Admin rights procedure",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3060
      },
      {
        "answerText": "Owner's rights procedure",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3059
      },
      {
        "answerText": "Caller's rights procedure",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3058
      }
    ]
  },
  {
    "id": 545,
    "questionText": "What is the purpose of network policies in Snowflake?",
    "questionType": "single",
    "explanation": "Network policies in Snowflake restrict access based on IP addresses by defining ALLOWED_IP_LIST (whitelist) and BLOCKED_IP_LIST (blacklist) parameters. They can be applied at the account level or to specific users, providing granular network-level security. When both lists are specified, the blocked list takes precedence for overlapping ranges.\n\nWhy other options fail:\n\u2022 Manage data encryption settings - Encryption is handled automatically by Snowflake using AES-256 encryption. There's no network policy setting for encryption management.\n\u2022 Control bandwidth allocation - Snowflake does not provide bandwidth throttling or allocation controls. Network performance is managed by the cloud infrastructure.\n\u2022 Define network regions for data storage - Data storage regions are determined when creating the Snowflake account and cannot be changed via network policies. Data residency is controlled at account creation.\n\nKey exam tip: Network policies use CIDR notation for IP ranges. Remember that BLOCKED_IP_LIST has priority over ALLOWED_IP_LIST when IPs overlap.",
    "domain": "2",
    "topic": "Network Security",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Restrict access based on IP addresses",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3063
      },
      {
        "answerText": "Define network regions for data storage",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3065
      },
      {
        "answerText": "Control bandwidth allocation",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3064
      },
      {
        "answerText": "Manage data encryption settings",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3062
      }
    ]
  },
  {
    "id": 546,
    "questionText": "Which Snowflake function extracts a value from a JSON object using a path?",
    "questionType": "single",
    "explanation": "GET_PATH extracts values from semi-structured VARIANT, OBJECT, or ARRAY data using a path expression. The dot notation (e.g., column:key1:key2) is syntactic sugar that translates to GET_PATH internally. For arrays, use bracket notation with index numbers (e.g., column:array[0]).\n\nWhy other options fail:\n\u2022 JSON_EXTRACT - This function does not exist in Snowflake. It may be confused with MySQL or other database syntax.\n\u2022 PARSE_JSON - This function converts a JSON string into a VARIANT value but does not extract values from existing VARIANT data. It's used for parsing, not path-based extraction.\n\u2022 EXTRACT_VALUE - This is not a valid Snowflake function. Snowflake uses GET_PATH, GET, or dot notation for value extraction.\n\nKey exam tip: The dot notation (data:field) is equivalent to GET_PATH(data, 'field'). Use FLATTEN to expand arrays, and LATERAL FLATTEN for correlated expansion in queries.",
    "domain": "5",
    "topic": "Semi-structured Data",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "PARSE_JSON",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3068
      },
      {
        "answerText": "EXTRACT_VALUE",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3069
      },
      {
        "answerText": "JSON_EXTRACT",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3066
      },
      {
        "answerText": "GET_PATH",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3067
      }
    ]
  },
  {
    "id": 547,
    "questionText": "What is the retention period for query history in ACCOUNT_USAGE views?",
    "questionType": "single",
    "explanation": "ACCOUNT_USAGE views in the SNOWFLAKE database retain query history for 365 days (1 year), providing comprehensive historical data for auditing, cost analysis, and performance troubleshooting. This contrasts with INFORMATION_SCHEMA views which only provide real-time or short-term data.\n\nWhy other options fail:\n\u2022 7 days - This is the retention period for some INFORMATION_SCHEMA views and the QUERY_HISTORY table function, not ACCOUNT_USAGE.\n\u2022 30 days - This does not match any standard Snowflake retention period for query history.\n\u2022 90 days - This is the default Time Travel retention for Enterprise edition, but not the ACCOUNT_USAGE retention period.\n\nKey exam tip: ACCOUNT_USAGE views have a latency of 45 minutes to 3 hours, while INFORMATION_SCHEMA provides near-real-time data with shorter retention. Use ACCOUNT_USAGE for historical analysis and INFORMATION_SCHEMA for current state.",
    "domain": "1",
    "topic": "Account Usage",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "90 days",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3072
      },
      {
        "answerText": "7 days",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3070
      },
      {
        "answerText": "30 days",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3071
      },
      {
        "answerText": "365 days",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3073
      }
    ]
  },
  {
    "id": 548,
    "questionText": "Which command is used to unload data from a Snowflake table to a stage?",
    "questionType": "single",
    "explanation": "COPY INTO <location> unloads data from a Snowflake table to a stage (internal or external). The command supports various file formats (CSV, JSON, Parquet) and options like SINGLE=TRUE to create one file, MAX_FILE_SIZE to control file splits, and HEADER=TRUE to include column headers.\n\nWhy other options fail:\n\u2022 EXPORT TO - This command does not exist in Snowflake. It may be confused with other database systems.\n\u2022 UNLOAD TO - This is not a valid Snowflake command. Amazon Redshift uses UNLOAD, but Snowflake uses COPY INTO for both loading and unloading.\n\u2022 WRITE TO - This command does not exist in Snowflake SQL syntax.\n\nKey exam tip: COPY INTO is used for both loading (COPY INTO <table>) and unloading (COPY INTO <stage>). The direction is determined by whether the target is a table or a stage location.",
    "domain": "4",
    "topic": "Data Unloading",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "EXPORT TO",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3074
      },
      {
        "answerText": "UNLOAD TO",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3075
      },
      {
        "answerText": "COPY INTO",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3076
      },
      {
        "answerText": "WRITE TO",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3077
      }
    ]
  },
  {
    "id": 549,
    "questionText": "What is the purpose of Row Access Policies in Snowflake?",
    "questionType": "single",
    "explanation": "Row Access Policies implement row-level security by dynamically filtering which rows are visible based on the querying user's context (role, user name, or custom attributes). The policy is defined once and automatically applied to all queries on the protected table, ensuring consistent security enforcement without modifying application queries.\n\nWhy other options fail:\n\u2022 Limit the number of rows returned by queries - Row limits are handled by LIMIT clauses in SQL, not Row Access Policies. Policies filter based on security context, not quantity.\n\u2022 Improve query performance by skipping rows - While policies do skip rows, their purpose is security, not performance. Partition pruning and clustering are used for performance optimization.\n\u2022 Encrypt specific rows in a table - Data encryption in Snowflake is automatic and applies to all data. For column-level masking, use Dynamic Data Masking policies, not Row Access Policies.\n\nKey exam tip: Row Access Policies use CURRENT_ROLE() or CURRENT_USER() to determine visibility. They are applied using ALTER TABLE ... ADD ROW ACCESS POLICY.",
    "domain": "2",
    "topic": "Row Access Policies",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Limit the number of rows returned by queries",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3078
      },
      {
        "answerText": "Improve query performance by skipping rows",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3080
      },
      {
        "answerText": "Encrypt specific rows in a table",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3081
      },
      {
        "answerText": "Filter visible rows based on user context",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3079
      }
    ]
  },
  {
    "id": 550,
    "questionText": "Which Snowflake layer handles query compilation and optimization?",
    "questionType": "single",
    "explanation": "The Cloud Services layer is Snowflake's brain, handling query parsing, compilation, optimization, and execution planning. It also manages authentication, access control, metadata, transaction management, and infrastructure orchestration. This layer runs continuously and handles up to 10% of daily compute costs for free (Cloud Services Adjustment).\n\nWhy other options fail:\n\u2022 Compute layer - The compute layer (virtual warehouses) executes the optimized query plans created by Cloud Services. It processes data but doesn't compile or optimize queries.\n\u2022 Storage layer - The storage layer persists data in cloud object storage (S3, Azure Blob, GCS). It handles data storage and retrieval but not query processing logic.\n\u2022 Network layer - This is not one of Snowflake's three architectural layers. Snowflake's architecture consists of Storage, Compute, and Cloud Services.\n\nKey exam tip: Snowflake's three-layer architecture (Storage, Compute, Cloud Services) enables independent scaling. Query compilation happens in Cloud Services; execution happens in Compute.",
    "domain": "1",
    "topic": "Architecture",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Compute layer",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3082
      },
      {
        "answerText": "Storage layer",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3083
      },
      {
        "answerText": "Cloud Services layer",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3084
      },
      {
        "answerText": "Network layer",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3085
      }
    ]
  },
  {
    "id": 551,
    "questionText": "What is the default authentication method for Snowflake users?",
    "questionType": "single",
    "explanation": "Username and password is Snowflake's default authentication method when creating users. Passwords must meet complexity requirements and can be rotated. Snowflake additionally supports Multi-Factor Authentication (MFA), SAML-based SSO, OAuth, and RSA key pair authentication for programmatic access.\n\nWhy other options fail:\n\u2022 Certificate-based authentication - Snowflake uses RSA key pair authentication (not X.509 certificates) for programmatic access. This must be explicitly configured and is not the default.\n\u2022 OAuth only - OAuth is supported but must be configured through security integrations. It's not the default method; username/password is available immediately.\n\u2022 Biometric authentication - Snowflake does not directly support biometric authentication. Biometrics would need to be handled by an external identity provider through SSO integration.\n\nKey exam tip: MFA enrollment is enforced via the MINS_TO_BYPASS_MFA_REGISTRATION parameter. Key pair authentication is recommended for service accounts and automated processes.",
    "domain": "2",
    "topic": "Authentication",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "OAuth only",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3088
      },
      {
        "answerText": "Username and password",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3087
      },
      {
        "answerText": "Biometric authentication",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3089
      },
      {
        "answerText": "Certificate-based authentication",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3086
      }
    ]
  },
  {
    "id": 552,
    "questionText": "Which feature enables automatic refreshing of data in a table based on a defined query?",
    "questionType": "single",
    "explanation": "Dynamic Tables automatically refresh their data based on a TARGET_LAG setting, which defines the maximum staleness allowed. You specify the transformation query declaratively, and Snowflake handles incremental refresh scheduling automatically. This simplifies ETL pipelines by eliminating the need to manage tasks and streams manually.\n\nWhy other options fail:\n\u2022 Materialized Views - Materialized Views are automatically and synchronously updated when base data changes, but they have limited query support (no joins across tables, limited aggregations). Dynamic Tables support more complex transformations.\n\u2022 Tasks - Tasks execute SQL on a schedule but require manual orchestration with streams for incremental processing. Dynamic Tables handle this automatically based on target lag.\n\u2022 Streams - Streams capture change data (CDC) but don't perform transformations or refresh tables automatically. They're often used with tasks, whereas Dynamic Tables combine both concepts.\n\nKey exam tip: Dynamic Tables use TARGET_LAG (e.g., '1 hour', '5 minutes') to control freshness. They can chain together for multi-stage ETL pipelines with automatic dependency management.",
    "domain": "5",
    "topic": "Dynamic Tables",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Dynamic Tables",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3091
      },
      {
        "answerText": "Streams",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3093
      },
      {
        "answerText": "Tasks",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3092
      },
      {
        "answerText": "Materialized Views",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3090
      }
    ]
  },
  {
    "id": 553,
    "questionText": "What is the purpose of the VALIDATION_MODE parameter in COPY INTO?",
    "questionType": "single",
    "explanation": "VALIDATION_MODE in COPY INTO validates data files without loading them into the target table. It can return the first N errors (RETURN_N_ROWS) or all errors (RETURN_ALL_ERRORS), helping identify data quality issues before committing to a production load. No data is inserted when validation mode is used.\n\nWhy other options fail:\n\u2022 Specify encryption validation - Encryption is handled automatically by Snowflake and doesn't require validation parameters. VALIDATION_MODE checks data content, not encryption.\n\u2022 Set data type validation rules - Data type validation happens automatically during loading based on the table schema. VALIDATION_MODE controls whether to load or just validate, not the validation rules themselves.\n\u2022 Validate user permissions - Permission validation happens before the COPY command executes. VALIDATION_MODE is specifically for checking data file content compatibility.\n\nKey exam tip: Use VALIDATION_MODE = 'RETURN_ERRORS' to find all issues, or 'RETURN_N_ROWS' to validate and preview rows. The VALIDATE() table function can also analyze past load results.",
    "domain": "4",
    "topic": "Data Validation",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Validate user permissions",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3097
      },
      {
        "answerText": "Set data type validation rules",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3096
      },
      {
        "answerText": "Validate data without loading it",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3095
      },
      {
        "answerText": "Specify encryption validation",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3094
      }
    ]
  },
  {
    "id": 554,
    "questionText": "Which system-defined role cannot be dropped or renamed?",
    "questionType": "multi",
    "explanation": "System-defined roles are built-in roles that Snowflake creates automatically and protects from modification. ACCOUNTADMIN, SECURITYADMIN, SYSADMIN, USERADMIN, PUBLIC, and ORGADMIN cannot be dropped, renamed, or have their core privileges revoked. These roles form the foundation of Snowflake's RBAC hierarchy.\n\nWhy DATA_ENGINEER is not correct:\n\u2022 DATA_ENGINEER is a custom role that organizations create themselves. It is not a system-defined role and can be dropped, renamed, or modified like any other custom role.\n\nRole hierarchy: ACCOUNTADMIN sits at the top, with SECURITYADMIN and SYSADMIN below it. SECURITYADMIN manages security-related objects and grants, while SYSADMIN manages warehouses, databases, and other objects. USERADMIN manages users and roles. PUBLIC is granted to all users automatically.\n\nKey exam tip: ACCOUNTADMIN should be used sparingly and protected with MFA. Best practice is to use SYSADMIN for day-to-day administrative tasks and SECURITYADMIN for access control.",
    "domain": "2",
    "topic": "System Roles",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "SECURITYADMIN",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3099
      },
      {
        "answerText": "DATA_ENGINEER",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3101
      },
      {
        "answerText": "ACCOUNTADMIN",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3098
      },
      {
        "answerText": "SYSADMIN",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3100
      }
    ]
  },
  {
    "id": 555,
    "questionText": "What is the maximum size of a single VARIANT value in Snowflake?",
    "questionType": "single",
    "explanation": "A single VARIANT value in Snowflake can store up to 16 MB of compressed data. This limit applies to the entire semi-structured document (JSON, Avro, Parquet, XML) stored in one cell. VARIANT data is stored in a columnar format with automatic type optimization for efficient querying.\n\nWhy other options fail:\n\u2022 1 MB - This is too small and doesn't match Snowflake's VARIANT limit. Some databases have smaller JSON limits, but Snowflake supports larger documents.\n\u2022 8 MB - This is half the actual limit. Snowflake specifically allows up to 16 MB per VARIANT value.\n\u2022 64 MB - This exceeds the actual limit. While Snowflake handles large data efficiently, individual VARIANT cells are capped at 16 MB.\n\nKey exam tip: If your JSON documents exceed 16 MB, consider splitting them across multiple rows or extracting large arrays into separate tables using FLATTEN. The 16 MB limit is per cell, not per row.",
    "domain": "1",
    "topic": "Data Types",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "16 MB",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3104
      },
      {
        "answerText": "1 MB",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3102
      },
      {
        "answerText": "8 MB",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3103
      },
      {
        "answerText": "64 MB",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3105
      }
    ]
  },
  {
    "id": 556,
    "questionText": "Which Snowflake feature allows querying data in external cloud storage without loading it?",
    "questionType": "single",
    "explanation": "External tables enable querying data files stored in external cloud storage (AWS S3, Azure Blob Storage, Google Cloud Storage) without copying the data into Snowflake's managed storage. The data remains in its original location, providing a 'data lake' query pattern with Snowflake's SQL capabilities. External tables can be auto-refreshed when new files arrive.\n\nWhy other options fail:\n\u2022 Data Lake - This is a general architecture concept, not a specific Snowflake feature. External tables are the mechanism Snowflake uses to query data lake files.\n\u2022 External Stages - Stages are storage locations referenced in COPY commands for loading/unloading data. They don't provide direct query capability; you need external tables to query staged data.\n\u2022 Data Shares - Data Sharing is for sharing Snowflake-managed data between accounts. It doesn't involve external cloud storage or data lakes.\n\nKey exam tip: External tables are read-only and generally slower than native tables due to external storage access. Use them for infrequent queries on data that must stay in the data lake, or as a staging approach before loading into native tables.",
    "domain": "4",
    "topic": "External Tables",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Data Lake",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3106
      },
      {
        "answerText": "Data Shares",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3109
      },
      {
        "answerText": "External Tables",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3107
      },
      {
        "answerText": "External Stages",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3108
      }
    ]
  },
  {
    "id": 557,
    "questionText": "What is the billing unit for Snowflake compute resources?",
    "questionType": "single",
    "explanation": "Snowflake bills compute resources using credits as the billing unit. Credit consumption is based on warehouse size and running time, with minimum billing in 60-second increments. An X-Small warehouse consumes 1 credit per hour, with each size increase doubling consumption (Small=2, Medium=4, Large=8, etc.).\n\nWhy other options fail:\n\u2022 Hours - While time is a factor in credit calculation, 'hours' is not the billing unit. Credits combine time and compute size into a single metric.\n\u2022 Bytes processed - Snowflake does not charge based on data scanned like some other cloud data warehouses. Storage is billed separately by TB-month.\n\u2022 Query count - The number of queries executed doesn't directly determine cost. A few complex queries could cost more than many simple ones based on warehouse usage time.\n\nKey exam tip: Storage and compute are billed separately in Snowflake. Compute uses credits; storage uses compressed TB-months. Serverless features (Snowpipe, tasks) have their own credit rates.",
    "domain": "1",
    "topic": "Billing",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Bytes processed",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3112
      },
      {
        "answerText": "Credits",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3111
      },
      {
        "answerText": "Hours",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3110
      },
      {
        "answerText": "Query count",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3113
      }
    ]
  },
  {
    "id": 558,
    "questionText": "Which parameter controls the automatic resumption of a suspended warehouse?",
    "questionType": "single",
    "explanation": "AUTO_RESUME, when set to TRUE (the default), automatically starts a suspended warehouse when a query is submitted against it. This enables on-demand compute without requiring users to manually resume warehouses, creating a seamless experience where compute resources appear instantly available.\n\nWhy other options fail:\n\u2022 AUTO_START - This parameter does not exist in Snowflake. The correct parameter name is AUTO_RESUME.\n\u2022 AUTO_ACTIVATE - This is not a valid Snowflake warehouse parameter. Snowflake uses AUTO_RESUME for this functionality.\n\u2022 RESUME_ON_DEMAND - This parameter does not exist. While it describes the functionality accurately, the actual parameter is named AUTO_RESUME.\n\nKey exam tip: AUTO_RESUME and AUTO_SUSPEND work together for cost optimization. Set AUTO_SUSPEND to control idle timeout (minimum 60 seconds) and leave AUTO_RESUME=TRUE so warehouses start automatically when needed.",
    "domain": "3",
    "topic": "Warehouse Configuration",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "AUTO_START",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3114
      },
      {
        "answerText": "RESUME_ON_DEMAND",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3117
      },
      {
        "answerText": "AUTO_ACTIVATE",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3116
      },
      {
        "answerText": "AUTO_RESUME",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3115
      }
    ]
  },
  {
    "id": 559,
    "questionText": "What does the MERGE statement do in Snowflake?",
    "questionType": "single",
    "explanation": "MERGE performs conditional INSERT, UPDATE, and DELETE operations in a single atomic statement by comparing source and target tables using join conditions. WHEN MATCHED handles existing records (update or delete), while WHEN NOT MATCHED handles new records (insert). This is essential for upsert patterns and slowly changing dimension (SCD) implementations.\n\nWhy other options fail:\n\u2022 Combines two tables into one - This describes UNION or table consolidation, not MERGE. MERGE modifies an existing target table based on source data, not combining tables.\n\u2022 Joins multiple queries together - This describes UNION, INTERSECT, or EXCEPT set operations. MERGE is a DML statement for data modification, not query combination.\n\u2022 Merges data from different databases - Cross-database operations in Snowflake use fully qualified names. MERGE is about conditional DML logic, not database-level data movement.\n\nKey exam tip: MERGE can have multiple WHEN MATCHED clauses with different conditions. For CDC processing, combine streams with MERGE for efficient incremental updates to target tables.",
    "domain": "5",
    "topic": "DML Operations",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Merges data from different databases",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3121
      },
      {
        "answerText": "Combines two tables into one",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3118
      },
      {
        "answerText": "Performs insert, update, and delete in one statement",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3119
      },
      {
        "answerText": "Joins multiple queries together",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3120
      }
    ]
  },
  {
    "id": 560,
    "questionText": "Which Snowflake feature provides a public marketplace for discovering and accessing shared data from third-party providers?",
    "questionType": "single",
    "explanation": "Snowflake Marketplace is a public data marketplace integrated into Snowsight where data providers list datasets, applications, and data services for discovery by any Snowflake customer. Consumers can instantly access free listings or request paid data products. Data is shared without copying, providing real-time access to provider-managed data.\n\nWhy other options fail:\n\u2022 Data Exchange - Data Exchange creates private sharing groups between specific, invited Snowflake accounts. Unlike Marketplace, it's not publicly discoverable; members must be explicitly added by the exchange admin.\n\u2022 Partner Connect - Partner Connect enables quick integration with third-party tools and services (BI tools, ETL platforms) but doesn't provide data sharing or marketplace functionality.\n\u2022 Data Hub - This is not a Snowflake feature name. The correct term for the public marketplace is Snowflake Marketplace.\n\nKey exam tip: Marketplace uses Secure Data Sharing under the hood - consumers query provider data without copying. Data providers can offer free, paid, or personalized listings through the Marketplace.",
    "domain": "6",
    "topic": "Marketplace",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Partner Connect",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3124
      },
      {
        "answerText": "Data Exchange",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3122
      },
      {
        "answerText": "Data Hub",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3125
      },
      {
        "answerText": "Snowflake Marketplace",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3123
      }
    ]
  },
  {
    "id": 561,
    "questionText": "What is the purpose of OBJECT_AGG function in Snowflake?",
    "questionType": "single",
    "explanation": "OBJECT_AGG is an aggregate function that combines key-value pairs from multiple rows into a single JSON object (OBJECT type). It takes two arguments: the key expression and the value expression. This is powerful for pivoting relational data into nested JSON structures or creating dynamic key-value mappings from grouped data.\n\nWhy other options fail:\n\u2022 Count objects in a column - Counting is done with COUNT() or COUNT_IF() functions. OBJECT_AGG creates objects, it doesn't count them.\n\u2022 Calculate object storage size - Storage size calculations use functions like LENGTH() for strings or metadata views for table storage. OBJECT_AGG doesn't measure size.\n\u2022 Compare objects for equality - Object comparison uses = operator or OBJECT_KEYS/FLATTEN for deep comparison. OBJECT_AGG is for construction, not comparison.\n\nKey exam tip: OBJECT_AGG is often used with GROUP BY to create JSON objects per group. Pair it with ARRAY_AGG to build complex nested JSON structures from relational data.",
    "domain": "5",
    "topic": "Aggregate Functions",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Compare objects for equality",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3129
      },
      {
        "answerText": "Calculate object storage size",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3128
      },
      {
        "answerText": "Aggregate key-value pairs into a JSON object",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3127
      },
      {
        "answerText": "Count objects in a column",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3126
      }
    ]
  },
  {
    "id": 562,
    "questionText": "Which Snowflake feature provides a web-based interface for data exploration and SQL development?",
    "questionType": "single",
    "explanation": "Snowsight is Snowflake's modern, feature-rich web interface that provides SQL worksheets with autocomplete, inline data visualization and charting, dashboard creation, database object browsing, query history, and administrative capabilities. It replaced the Classic Console as the primary UI and offers improved collaboration features like worksheet sharing.\n\nWhy other options fail:\n\u2022 SnowSQL - SnowSQL is Snowflake's command-line interface (CLI) for executing SQL from a terminal. It's text-based, not a web interface, and is used for scripting and automation.\n\u2022 Classic Console - The Classic Console was Snowflake's original web UI, now deprecated in favor of Snowsight. It lacks the modern features and visualization capabilities of Snowsight.\n\u2022 Data Studio - This is not a Snowflake product. Google has a product called Looker Studio (formerly Data Studio), but Snowflake's web interface is called Snowsight.\n\nKey exam tip: Snowsight provides features not available in SnowSQL, including visual query profiling, chart creation, and dashboard sharing. SnowSQL is better for automation and scripting scenarios.",
    "domain": "1",
    "topic": "Interfaces",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Snowsight",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3131
      },
      {
        "answerText": "SnowSQL",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3130
      },
      {
        "answerText": "Classic Console",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3132
      },
      {
        "answerText": "Data Studio",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3133
      }
    ]
  },
  {
    "id": 563,
    "questionText": "What is the purpose of the STRIP_OUTER_ARRAY file format option?",
    "questionType": "single",
    "explanation": "STRIP_OUTER_ARRAY removes the outer JSON array brackets when loading JSON files, treating each array element as a separate row. This is essential when your JSON file contains an array of objects like [{...}, {...}, {...}] and you want each object to become a separate table row rather than loading the entire array as a single VARIANT value.\n\nWhy other options fail:\n\u2022 Remove whitespace from arrays: There is no file format option specifically for removing whitespace from arrays; compression and parsing handle formatting automatically.\n\u2022 Flatten nested arrays automatically: FLATTEN function handles nested arrays during querying, not STRIP_OUTER_ARRAY which only removes the outermost brackets during load.\n\u2022 Remove empty array elements: NULL handling options like STRIP_NULL_VALUES deal with null/empty values, not STRIP_OUTER_ARRAY.\n\nKey exam tip: STRIP_OUTER_ARRAY only affects the outermost array brackets. Nested arrays within each element remain intact and require FLATTEN for processing.",
    "domain": "4",
    "topic": "File Format Options",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Flatten nested arrays automatically",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3136
      },
      {
        "answerText": "Load each array element as a separate row",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3135
      },
      {
        "answerText": "Remove empty array elements",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3137
      },
      {
        "answerText": "Remove whitespace from arrays",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3134
      }
    ]
  },
  {
    "id": 564,
    "questionText": "Which Snowflake function returns the current warehouse name?",
    "questionType": "single",
    "explanation": "CURRENT_WAREHOUSE() is one of Snowflake's context functions that returns the name of the warehouse currently in use for the session. If no warehouse is selected (USE WAREHOUSE not executed), it returns NULL. This function is useful in stored procedures, scripts, and logging to track which warehouse executed a query.\n\nWhy other options fail:\n\u2022 GET_WAREHOUSE(): This function does not exist in Snowflake. Context functions follow the CURRENT_* naming pattern.\n\u2022 SESSION_WAREHOUSE(): Not a valid Snowflake function. Session-related information uses CURRENT_SESSION() instead.\n\u2022 ACTIVE_WAREHOUSE(): Not a valid Snowflake function. The correct naming convention is CURRENT_*, not ACTIVE_*.\n\nKey exam tip: Snowflake context functions follow the CURRENT_* pattern: CURRENT_WAREHOUSE(), CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_ROLE(), CURRENT_USER(), CURRENT_SESSION(), CURRENT_ACCOUNT().",
    "domain": "5",
    "topic": "Context Functions",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "SESSION_WAREHOUSE()",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3140
      },
      {
        "answerText": "GET_WAREHOUSE()",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3138
      },
      {
        "answerText": "CURRENT_WAREHOUSE()",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3139
      },
      {
        "answerText": "ACTIVE_WAREHOUSE()",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3141
      }
    ]
  },
  {
    "id": 565,
    "questionText": "What is the purpose of the USERADMIN role in Snowflake?",
    "questionType": "single",
    "explanation": "USERADMIN is one of Snowflake's system-defined roles specifically designed for user and role management. It can CREATE USER, CREATE ROLE, and manage custom roles, but importantly cannot grant privileges to those objects. SECURITYADMIN inherits all USERADMIN privileges and adds the ability to manage grants (GRANT/REVOKE), making it the role for complete access control management.\n\nWhy other options fail:\n\u2022 Create and manage databases: Database creation and management is handled by SYSADMIN, not USERADMIN. USERADMIN has no privileges on data objects.\n\u2022 Monitor user activity: Monitoring requires access to ACCOUNT_USAGE views or INFORMATION_SCHEMA, which USERADMIN does not have by default. ACCOUNTADMIN typically handles monitoring.\n\u2022 Manage user sessions: Session management (killing queries, viewing active sessions) requires ACCOUNTADMIN privileges or specific grants, not USERADMIN.\n\nKey exam tip: Remember the role hierarchy: ACCOUNTADMIN > SECURITYADMIN > USERADMIN. USERADMIN creates users/roles, SECURITYADMIN manages grants, and SYSADMIN manages objects.",
    "domain": "2",
    "topic": "System Roles",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Create and manage users and roles",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3143
      },
      {
        "answerText": "Create and manage databases",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3142
      },
      {
        "answerText": "Monitor user activity",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3144
      },
      {
        "answerText": "Manage user sessions",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3145
      }
    ]
  },
  {
    "id": 566,
    "questionText": "Which clause is used to specify partition columns when creating an external table?",
    "questionType": "single",
    "explanation": "PARTITION BY clause is used when creating external tables to define partition columns that extract values from the file path structure in the external stage. For example, if files are organized as /year=2024/month=01/data.parquet, PARTITION BY can extract year and month as virtual columns. This enables partition pruning, where Snowflake only scans relevant file paths based on query filters, dramatically improving query performance.\n\nWhy other options fail:\n\u2022 CLUSTER BY: This clause is used for clustering keys on regular Snowflake tables to improve micro-partition organization, not for external table partitioning.\n\u2022 ORGANIZE BY: Not a valid Snowflake clause. This syntax does not exist in Snowflake DDL.\n\u2022 DISTRIBUTE BY: Not a valid Snowflake clause. This is terminology from other systems like Spark or Hive, not Snowflake.\n\nKey exam tip: External table partitions are inferred from the directory path structure using expressions like SPLIT_PART(METADATA$FILENAME, '/', n). This differs from regular table clustering.",
    "domain": "4",
    "topic": "External Tables",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "DISTRIBUTE BY",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3149
      },
      {
        "answerText": "ORGANIZE BY",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3148
      },
      {
        "answerText": "CLUSTER BY",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3146
      },
      {
        "answerText": "PARTITION BY",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3147
      }
    ]
  },
  {
    "id": 567,
    "questionText": "What does the EXPLAIN command do in Snowflake?",
    "questionType": "single",
    "explanation": "EXPLAIN displays the logical execution plan for a query without actually executing it, consuming no compute resources. It shows the sequence of operations Snowflake will perform including table scans, joins, aggregations, and filter operations. The output reveals the query structure, helping identify potential performance issues before execution. Use EXPLAIN USING TEXT for readable output or EXPLAIN USING TABULAR for structured results.\n\nWhy other options fail:\n\u2022 Documents the query purpose: EXPLAIN shows technical execution details, not documentation. Comments (--) or query history serve documentation purposes.\n\u2022 Explains query errors: EXPLAIN only works on valid queries. Syntax or semantic errors must be fixed before EXPLAIN can generate a plan.\n\u2022 Shows query cost estimate: EXPLAIN shows the execution plan structure, not cost estimates. Cost/credit estimates require running the query or using Query Profile in the UI after execution.\n\nKey exam tip: EXPLAIN is a planning tool that uses no warehouse compute. For actual execution statistics and performance metrics, use Query Profile from query history after running the query.",
    "domain": "3",
    "topic": "Query Optimization",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Documents the query purpose",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3150
      },
      {
        "answerText": "Shows query cost estimate",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3153
      },
      {
        "answerText": "Explains query errors",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3152
      },
      {
        "answerText": "Shows the query execution plan",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3151
      }
    ]
  },
  {
    "id": 568,
    "questionText": "Which type of stream tracks all changes including metadata columns for CDC?",
    "questionType": "single",
    "explanation": "Standard streams (the default type) track all DML changes including INSERTs, UPDATEs, and DELETEs, providing comprehensive CDC (Change Data Capture) capabilities. They include metadata columns: METADATA$ACTION (INSERT or DELETE), METADATA$ISUPDATE (true if part of an UPDATE), and METADATA$ROW_ID (unique row identifier). Updates appear as two rows: a DELETE of the old value and INSERT of the new value, both with METADATA$ISUPDATE = TRUE.\n\nWhy other options fail:\n\u2022 Append-only stream: This stream type only tracks INSERT operations, ignoring UPDATEs and DELETEs. It is optimized for append-only workloads like log data but lacks full CDC capability.\n\u2022 Insert-only stream: This is not a valid Snowflake stream type. The correct term is \"append-only stream.\"\n\u2022 Delta stream: This is not a valid Snowflake stream type. Snowflake offers Standard, Append-only, and Insert-only (for directory tables) stream types.\n\nKey exam tip: Standard streams consume more storage because they track all changes. Use append-only streams for pure insert workloads (like logging) to improve efficiency.",
    "domain": "5",
    "topic": "Streams",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Append-only stream",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3154
      },
      {
        "answerText": "Standard stream",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3155
      },
      {
        "answerText": "Delta stream",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3157
      },
      {
        "answerText": "Insert-only stream",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3156
      }
    ]
  },
  {
    "id": 569,
    "questionText": "What is the purpose of MFA (Multi-Factor Authentication) in Snowflake?",
    "questionType": "single",
    "explanation": "MFA (Multi-Factor Authentication) adds an essential security layer by requiring users to provide two forms of verification: something they know (password) and something they have (authentication app like Duo Mobile). Snowflake supports MFA enrollment per user via ALTER USER ... SET MINS_TO_BYPASS_MFA or account-level enforcement through authentication policies. MFA significantly reduces the risk of unauthorized access even if passwords are compromised.\n\nWhy other options fail:\n\u2022 Faster login process: MFA actually adds an extra step to authentication, making login slightly longer (but much more secure). Speed is not the purpose of MFA.\n\u2022 Multiple user access to same account: MFA is about authentication security, not multi-user access. Multiple users access accounts through separate user credentials, not MFA.\n\u2022 Automated password management: Password management is handled through password policies, not MFA. MFA supplements passwords rather than managing them.\n\nKey exam tip: Snowflake uses Duo Security for MFA. Users self-enroll through the Snowflake UI, and administrators can enforce MFA at the account level using authentication policies (Enterprise Edition+).",
    "domain": "2",
    "topic": "Authentication",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Multiple user access to same account",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3160
      },
      {
        "answerText": "Automated password management",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3161
      },
      {
        "answerText": "Additional security layer for user authentication",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3159
      },
      {
        "answerText": "Faster login process",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3158
      }
    ]
  },
  {
    "id": 570,
    "questionText": "What is the primary benefit of Snowflake's separation of storage and compute?",
    "questionType": "single",
    "explanation": "Snowflake's separation of storage and compute is a foundational architectural principle that enables independent scaling of each layer. You can store petabytes of data without provisioning any compute, and scale up warehouses during peak processing without affecting storage. This eliminates the traditional data warehouse problem of over-provisioning either resource to meet varying demands. Multiple warehouses can simultaneously access the same data without data movement or contention.\n\nWhy other options fail:\n\u2022 Faster query execution: While the architecture supports good performance, separation itself does not directly speed up queries. Warehouse size, caching, and query optimization determine speed.\n\u2022 Better data compression: Snowflake's columnar storage and automatic compression provide good compression, but this is independent of the storage-compute separation architecture.\n\u2022 Improved data security: Security features (encryption, RBAC, network policies) are separate from the storage-compute architecture. The separation is about scalability and cost optimization.\n\nKey exam tip: This separation enables key Snowflake benefits: pay only for storage when idle, instant elasticity for compute, zero-copy cloning (storage layer), and concurrent workload isolation (compute layer).",
    "domain": "1",
    "topic": "Architecture",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Better data compression",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3164
      },
      {
        "answerText": "Faster query execution",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3162
      },
      {
        "answerText": "Independent scaling of resources",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3163
      },
      {
        "answerText": "Improved data security",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3165
      }
    ]
  },
  {
    "id": 571,
    "questionText": "Which command shows the status of files in a stage?",
    "questionType": "single",
    "explanation": "LIST (alias: LS) displays metadata about files in a stage, showing the file name (path), size in bytes, MD5 hash, and last modified timestamp. It works with all stage types: user stages (@~), table stages (@%table_name), named internal stages (@stage_name), and external stages. The command supports pattern matching to filter results (e.g., LIST @mystage PATTERN='.*csv').\n\nWhy other options fail:\n\u2022 SHOW FILES: Not a valid Snowflake command. SHOW is used for database objects (SHOW TABLES, SHOW STAGES) but not for listing file contents within stages.\n\u2022 DIR: Not a valid Snowflake command. This is a Windows/DOS command, not SQL syntax.\n\u2022 GET FILES: Not valid syntax. GET is a command for downloading files from a stage to a local machine, not for listing files.\n\nKey exam tip: LIST shows staged files, while SHOW STAGES shows stage objects. Use LIST @stage_name PATTERN='.*\\.csv' to filter for specific file types. The PATTERN uses regex, not glob patterns.",
    "domain": "4",
    "topic": "Stages",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "LIST",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3167
      },
      {
        "answerText": "DIR",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3168
      },
      {
        "answerText": "GET FILES",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3169
      },
      {
        "answerText": "SHOW FILES",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3166
      }
    ]
  },
  {
    "id": 572,
    "questionText": "What is the default behavior when a task is created in Snowflake?",
    "questionType": "single",
    "explanation": "Tasks are always created in a SUSPENDED state by default as a safety mechanism. This prevents accidental execution of scheduled SQL statements before proper testing and validation. To activate a task, you must explicitly run ALTER TASK task_name RESUME. For task trees (tasks with dependencies), you must resume tasks in reverse dependency order: child tasks first, then the root task.\n\nWhy other options fail:\n\u2022 Task starts running immediately: This would be dangerous as untested tasks could execute unintended operations. The suspended default ensures deliberate activation.\n\u2022 Task runs once and then suspends: Tasks continue running on their schedule until explicitly suspended. There is no single-execution default mode.\n\u2022 Task waits for manual trigger: While tasks CAN be triggered manually with EXECUTE TASK, the question asks about default creation behavior, which is suspended (not waiting for trigger).\n\nKey exam tip: When working with task trees, resume child tasks before the root task. To suspend, reverse the order: suspend root first, then children. Only the root task needs a warehouse; child tasks inherit it.",
    "domain": "5",
    "topic": "Tasks",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Task starts running immediately",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3170
      },
      {
        "answerText": "Task runs once and then suspends",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3172
      },
      {
        "answerText": "Task waits for manual trigger",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3173
      },
      {
        "answerText": "Task is created in suspended state",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3171
      }
    ]
  },
  {
    "id": 573,
    "questionText": "Which Snowflake edition is required for Private Link connectivity?",
    "questionType": "single",
    "explanation": "AWS PrivateLink and Azure Private Link are network security features that require Business Critical Edition or higher. Private Link creates a private endpoint in your VPC/VNet that connects directly to Snowflake's internal network, ensuring that traffic never traverses the public internet. This is essential for organizations with strict security requirements, compliance mandates (HIPAA, PCI-DSS), or data residency concerns.\n\nWhy other options fail:\n\u2022 Standard Edition: Standard Edition provides basic Snowflake features but lacks advanced security features like Private Link, Tri-Secret Secure encryption, and customer-managed keys.\n\u2022 Enterprise Edition: Enterprise Edition adds features like multi-cluster warehouses, materialized views, and Time Travel up to 90 days, but Private Link requires the next tier (Business Critical).\n\u2022 All editions: Private Link is explicitly a Business Critical+ feature due to its enterprise security nature and infrastructure requirements.\n\nKey exam tip: Business Critical Edition security features include: Private Link (AWS/Azure), Tri-Secret Secure (customer-managed keys), HIPAA/PCI-DSS compliance support, and PHI data support. GCP uses Private Service Connect.",
    "domain": "2",
    "topic": "Network Security",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Standard Edition",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3174
      },
      {
        "answerText": "Enterprise Edition",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3175
      },
      {
        "answerText": "Business Critical Edition",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3176
      },
      {
        "answerText": "All editions",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3177
      }
    ]
  },
  {
    "id": 574,
    "questionText": "What is the maximum number of columns a Snowflake table can have?",
    "questionType": "single",
    "explanation": "Snowflake tables support a maximum of 10,000 columns per table. This limit accommodates wide, denormalized tables common in analytics workloads, IoT sensor data, or feature stores for machine learning. Snowflake's columnar storage makes wide tables efficient because queries only read the columns they need, and unused columns do not impact query performance.\n\nWhy other options fail:\n\u2022 1,000 columns: This is lower than the actual limit. While many traditional databases have lower limits, Snowflake's architecture supports wider tables.\n\u2022 100,000 columns: This exceeds Snowflake's actual limit. No current Snowflake table can have 100,000 columns.\n\u2022 Unlimited: Snowflake has defined limits on most objects for system stability and performance. The 10,000 column limit is a documented constraint.\n\nKey exam tip: While Snowflake allows 10,000 columns, consider using VARIANT columns for highly dynamic schemas with many attributes. This provides flexibility without consuming column slots and is more efficient for sparse data.",
    "domain": "1",
    "topic": "Limits",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "10,000 columns",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3179
      },
      {
        "answerText": "1,000 columns",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3178
      },
      {
        "answerText": "Unlimited",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3181
      },
      {
        "answerText": "100,000 columns",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3180
      }
    ]
  },
  {
    "id": 575,
    "questionText": "Which function converts a timestamp to a specified timezone?",
    "questionType": "single",
    "explanation": "CONVERT_TIMEZONE converts a timestamp from one timezone to another. It supports two syntaxes: CONVERT_TIMEZONE(target_tz, source_timestamp) assumes the source is in the session timezone, while CONVERT_TIMEZONE(source_tz, target_tz, source_timestamp) explicitly specifies the source timezone. For TIMESTAMP_NTZ inputs, it assumes the session timezone. For TIMESTAMP_TZ inputs, it uses the embedded timezone information.\n\nWhy other options fail:\n\u2022 TO_TIMEZONE: Not a valid Snowflake function. Conversion functions typically use CONVERT_* or TO_* prefix, but TO_TIMEZONE does not exist.\n\u2022 CHANGE_TIMEZONE: Not a valid Snowflake function. This syntax is not part of Snowflake's function library.\n\u2022 SET_TIMEZONE: Not a valid function for conversion. SET is used for session parameters (ALTER SESSION SET TIMEZONE), not for data transformation.\n\nKey exam tip: Know Snowflake's three timestamp types: TIMESTAMP_NTZ (no timezone, wall clock time), TIMESTAMP_LTZ (local timezone, stored as UTC), and TIMESTAMP_TZ (timezone-aware, stores timezone with value). CONVERT_TIMEZONE returns TIMESTAMP_TZ.",
    "domain": "5",
    "topic": "Date/Time Functions",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "SET_TIMEZONE",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3185
      },
      {
        "answerText": "CONVERT_TIMEZONE",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3183
      },
      {
        "answerText": "CHANGE_TIMEZONE",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3184
      },
      {
        "answerText": "TO_TIMEZONE",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3182
      }
    ]
  },
  {
    "id": 576,
    "questionText": "What is the purpose of the METADATA$FILENAME pseudocolumn in Snowflake?",
    "questionType": "single",
    "explanation": "METADATA$FILENAME is a pseudocolumn available during data loading that returns the full path and name of the staged file from which each row was loaded. This is invaluable for data lineage tracking (knowing which source file produced each row), debugging load issues, and auditing. You can load this value into a regular column during COPY INTO to persist the source file information.\n\nWhy other options fail:\n\u2022 Table name where data is stored: The target table name is specified in the COPY INTO command, not returned by a metadata column. Table metadata is accessed via INFORMATION_SCHEMA.\n\u2022 Output file name for exports: METADATA$FILENAME is for data loading (COPY INTO table), not data unloading (COPY INTO stage). Unload file names are controlled by COPY INTO options.\n\u2022 Stage name for the file: The stage name is part of the full path in METADATA$FILENAME, but this column returns the complete file path, not just the stage name.\n\nKey exam tip: Other useful metadata pseudocolumns include METADATA$FILE_ROW_NUMBER (row position in file), METADATA$FILE_CONTENT_KEY (unique file identifier), and METADATA$FILE_LAST_MODIFIED (file modification timestamp).",
    "domain": "4",
    "topic": "Metadata",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Table name where data is stored",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3186
      },
      {
        "answerText": "Stage name for the file",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3189
      },
      {
        "answerText": "Source file name during data load",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3187
      },
      {
        "answerText": "Output file name for exports",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3188
      }
    ]
  },
  {
    "id": 577,
    "questionText": "Which object type can be shared with data consumers through Secure Data Sharing?",
    "questionType": "multi",
    "explanation": "Secure Data Sharing allows providers to share specific object types with consumers: Tables (for direct data access), Secure Views (for filtered/transformed data with hidden logic), and Secure UDFs (for shared business logic and calculations). Shared objects are read-only for consumers, always reflect the provider's latest data in real-time (no data copying), and consumers use their own compute resources to query the shared data.\n\nWhy other options fail:\n\u2022 Warehouses: Warehouses are compute resources owned by each account and cannot be shared. Consumers must use their own warehouses to query shared data, ensuring providers do not bear compute costs.\n\nCorrect options explained:\n\u2022 Tables: Can be shared directly, giving consumers access to all table data unless restricted by secure views.\n\u2022 Secure Views: Enable row/column-level filtering and hide view definitions from consumers, essential for data governance.\n\u2022 Secure UDFs: Allow sharing custom functions while protecting the implementation logic from consumers.\n\nKey exam tip: Only SECURE views and UDFs can be shared (not regular views). This prevents consumers from reverse-engineering the underlying query logic through EXPLAIN plans or error messages.",
    "domain": "6",
    "topic": "Data Sharing",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Tables",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3190
      },
      {
        "answerText": "Secure UDFs",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3193
      },
      {
        "answerText": "Secure Views",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3191
      },
      {
        "answerText": "Warehouses",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3192
      }
    ]
  },
  {
    "id": 578,
    "questionText": "What is the purpose of the QUERY_ACCELERATION_MAX_SCALE_FACTOR parameter?",
    "questionType": "single",
    "explanation": "QUERY_ACCELERATION_MAX_SCALE_FACTOR is a warehouse parameter that controls the maximum additional serverless compute resources Query Acceleration Service (QAS) can allocate to accelerate eligible queries. The value is a multiplier of the warehouse size (0-100, where 0 disables QAS). For example, a value of 8 on a Medium warehouse allows QAS to use up to 8x the warehouse's compute capacity for acceleration. This helps queries with large scans, selective filters, or aggregations.\n\nWhy other options fail:\n\u2022 Maximum number of concurrent queries: Concurrency is controlled by warehouse size, multi-cluster settings (MIN/MAX_CLUSTER_COUNT), and query queuing, not by QAS scale factor.\n\u2022 Query result cache size multiplier: Result caching is automatic and managed by Snowflake's metadata layer, not controlled by this parameter.\n\u2022 Maximum warehouse auto-scaling factor: Multi-cluster warehouse scaling uses MIN/MAX_CLUSTER_COUNT parameters, not QUERY_ACCELERATION_MAX_SCALE_FACTOR.\n\nKey exam tip: Query Acceleration Service (QAS) is available on Enterprise Edition+. It offloads portions of eligible queries to shared serverless compute. Set scale factor to 0 to disable, or higher values to allow more acceleration capacity (and cost).",
    "domain": "3",
    "topic": "Query Acceleration",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Maximum additional compute for query acceleration",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3195
      },
      {
        "answerText": "Maximum number of concurrent queries",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3194
      },
      {
        "answerText": "Maximum warehouse auto-scaling factor",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3197
      },
      {
        "answerText": "Query result cache size multiplier",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3196
      }
    ]
  },
  {
    "id": 579,
    "questionText": "Which Snowflake feature enables manual data governance through user-defined metadata labels on database objects?",
    "questionType": "single",
    "explanation": "Object Tagging enables manual data governance by allowing users to create and apply custom metadata labels (tags) to database objects including databases, schemas, tables, views, and columns. Tags follow a key-value format and propagate through lineage (tag inheritance). When combined with tag-based masking policies, you can apply consistent security rules across all objects sharing a tag, enabling scalable governance without managing individual object policies.\n\nWhy other options fail:\n\u2022 Data Labels: Not a specific Snowflake feature name. While tags function as labels, the official feature is called Object Tagging.\n\u2022 Data Classification: This is a separate Snowflake feature that AUTOMATICALLY detects and classifies sensitive data (PII, PHI) using system tags. The question specifically asks about manual, user-defined metadata labels.\n\u2022 Metadata Management: This is a general concept, not a specific Snowflake feature. Snowflake provides metadata through INFORMATION_SCHEMA and ACCOUNT_USAGE, but tagging is the feature for user-defined metadata.\n\nKey exam tip: Distinguish between Object Tagging (manual, user-defined tags) and Data Classification (automatic, system-generated classification). Both support tag-based masking policies for column-level security.",
    "domain": "2",
    "topic": "Data Governance",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Object Tagging",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3199
      },
      {
        "answerText": "Data Classification",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3200
      },
      {
        "answerText": "Metadata Management",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3201
      },
      {
        "answerText": "Data Labels",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3198
      }
    ]
  },
  {
    "id": 580,
    "questionText": "What is the purpose of the SYSTEM$CLUSTERING_DEPTH function?",
    "questionType": "single",
    "explanation": "SYSTEM$CLUSTERING_DEPTH is a system function that measures how well a table's data is physically organized according to its clustering key. It returns the average depth of overlapping micro-partitions that must be scanned for a given clustering key value. A depth of 1 means perfect clustering (each key value exists in only one micro-partition), while higher values indicate more overlap and less effective partition pruning. Use this function to monitor clustering health and determine if reclustering is needed.\n\nWhy other options fail:\n\u2022 Number of clustering keys defined: SHOW TABLES or table DDL shows clustering key definitions. SYSTEM$CLUSTERING_DEPTH measures clustering quality, not the number of keys.\n\u2022 Maximum nesting depth for JSON: JSON depth is unrelated to table clustering. FLATTEN and recursive CTEs handle nested JSON structures.\n\u2022 Number of partitions in a table: SYSTEM$CLUSTERING_INFORMATION provides partition statistics. CLUSTERING_DEPTH specifically measures partition overlap quality.\n\nKey exam tip: Use SYSTEM$CLUSTERING_INFORMATION for detailed statistics including clustering depth, average overlap, and partition counts. Lower depth values are better. Automatic Clustering (Enterprise+) maintains optimal clustering without manual RECLUSTER commands.",
    "domain": "3",
    "topic": "Clustering",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Measure of how well data is clustered",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3203
      },
      {
        "answerText": "Number of clustering keys defined",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3202
      },
      {
        "answerText": "Maximum nesting depth for JSON",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3204
      },
      {
        "answerText": "Number of partitions in a table",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3205
      }
    ]
  },
  {
    "id": 581,
    "questionText": "How does Snowpipe handle duplicate files?",
    "questionType": "single",
    "explanation": "Snowpipe automatically prevents duplicate file loading by maintaining a 14-day load history that tracks files using their metadata: file path, size, and ETag (or checksum). If the same file (matching all three attributes) is staged again within 14 days, Snowpipe skips it automatically. This is built-in behavior requiring no configuration. If you modify a file and re-stage it with the same name, the new ETag/checksum will differ, and Snowpipe will load it as a new file.\n\nWhy other options fail:\n\u2022 SKIP_DUPLICATE_FILES: Not a valid Snowpipe parameter. Duplicate prevention is automatic and cannot be configured via parameters.\n\u2022 FORCE parameter: FORCE is used with regular COPY INTO commands to reload files, bypassing the load history. It is not a Snowpipe parameter and does not apply to continuous loading.\n\u2022 DUPLICATE_HANDLING: Not a valid Snowflake parameter. This option does not exist in Snowflake's data loading configuration.\n\nKey exam tip: The 14-day tracking window means files older than 14 days may be reloaded if re-staged. For COPY INTO (not Snowpipe), use FORCE = TRUE to intentionally reload files or rely on the 64-day default load history.",
    "domain": "4",
    "topic": "Snowpipe",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "DUPLICATE_HANDLING",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3209
      },
      {
        "answerText": "FORCE parameter",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3208
      },
      {
        "answerText": "Snowpipe automatically tracks files for 14 days",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3207
      },
      {
        "answerText": "SKIP_DUPLICATE_FILES",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3206
      }
    ]
  },
  {
    "id": 582,
    "questionText": "What is the purpose of a Sequence object in Snowflake?",
    "questionType": "single",
    "explanation": "A Sequence is a schema-level object that generates unique, sequential numeric values. Unlike AUTOINCREMENT (column property), sequences are independent objects that can be referenced by multiple tables, providing flexibility for surrogate key generation across related tables. Sequences support configurable START, INCREMENT values, and can be used in DEFAULT expressions or directly via sequence_name.NEXTVAL. Values are guaranteed unique but may have gaps due to caching for performance.\n\nWhy other options fail:\n\u2022 Define execution order for tasks: Task execution order is defined using the AFTER clause when creating dependent tasks, not sequences. Task dependencies form DAGs (directed acyclic graphs).\n\u2022 Track changes in a stream: Streams track DML changes using internal offset tracking, not sequences. Streams provide METADATA$ACTION, METADATA$ISUPDATE, and METADATA$ROW_ID columns.\n\u2022 Order query results: Query result ordering uses ORDER BY clause, not sequences. Sequences generate values during INSERT, not during SELECT operations.\n\nKey exam tip: Sequences may produce gaps in values (due to caching and rollbacks) and are not guaranteed to be gapless. For strictly sequential numbering, use ROW_NUMBER() window function at query time instead.",
    "domain": "5",
    "topic": "Sequences",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Generate unique sequential numbers",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3211
      },
      {
        "answerText": "Order query results",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3213
      },
      {
        "answerText": "Track changes in a stream",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3212
      },
      {
        "answerText": "Define execution order for tasks",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3210
      }
    ]
  },
  {
    "id": 583,
    "questionText": "Which Snowflake feature provides AI-powered data transformation capabilities?",
    "questionType": "single",
    "explanation": "Snowflake Cortex is Snowflake's integrated AI/ML platform that provides AI-powered data transformation capabilities directly within SQL. It includes Large Language Model (LLM) functions like COMPLETE, SUMMARIZE, TRANSLATE, and SENTIMENT for text processing, as well as ML Functions for tasks like classification and forecasting - all accessible via simple SQL syntax without external infrastructure.\n\nWhy other options fail:\n\u2022 Snowpark ML is a Python library for building and training ML models in Snowpark, not a native SQL-based AI transformation feature\n\u2022 AI Functions is not a specific Snowflake feature name - Cortex encompasses AI capabilities under a unified branded offering\n\u2022 Machine Learning Studio does not exist as a Snowflake product - this is a fabricated option\n\nKey exam tip: Remember that Cortex = AI/ML in Snowflake. It's the umbrella for all built-in AI capabilities including LLM functions, ML functions, and vector search.",
    "domain": "1",
    "topic": "Cortex",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Snowflake Cortex",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3215
      },
      {
        "answerText": "AI Functions",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3216
      },
      {
        "answerText": "Machine Learning Studio",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3217
      },
      {
        "answerText": "Snowpark ML",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3214
      }
    ]
  },
  {
    "id": 584,
    "questionText": "What is the purpose of the GET command in Snowflake?",
    "questionType": "single",
    "explanation": "The GET command downloads files from a Snowflake internal stage (user, table, or named stage) to a local file system. It is the counterpart to PUT (which uploads files) and can only be executed through SnowSQL CLI or Snowflake connectors - not through the web UI. GET supports parallel downloading and encryption for secure file transfer.\n\nWhy other options fail:\n\u2022 Retrieve data from a table - use SELECT statements to query table data, not GET\n\u2022 Get metadata about an object - use DESCRIBE or SHOW commands for metadata retrieval\n\u2022 Fetch query results - query results are returned directly from SELECT statements or accessed via result set APIs\n\nKey exam tip: PUT uploads TO stages, GET downloads FROM stages. Both require SnowSQL/connectors and work only with internal stages (not external stages like S3).",
    "domain": "4",
    "topic": "Data Unloading",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Retrieve data from a table",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3218
      },
      {
        "answerText": "Get metadata about an object",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3220
      },
      {
        "answerText": "Fetch query results",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3221
      },
      {
        "answerText": "Download files from a stage to local machine",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3219
      }
    ]
  },
  {
    "id": 585,
    "questionText": "Which privilege is required to create a share in Snowflake?",
    "questionType": "single",
    "explanation": "The CREATE SHARE global privilege is specifically required to create outbound shares in Snowflake. This privilege allows a role to create share objects and is typically held by ACCOUNTADMIN by default. Organizations often grant this to dedicated data sharing administrator roles to enable controlled data sharing without full account administration access.\n\nWhy other options fail:\n\u2022 USAGE on database - this grants access to use a database but does not permit creating shares; shares are account-level objects\n\u2022 MANAGE SHARES - this is not a valid Snowflake privilege; the correct privilege is CREATE SHARE\n\u2022 OWNERSHIP on tables - while you need privileges on objects to add them to a share, OWNERSHIP alone does not allow share creation\n\nKey exam tip: CREATE SHARE is a global (account-level) privilege, separate from object-level privileges. After creating a share, you still need appropriate privileges on objects to add them to the share.",
    "domain": "6",
    "topic": "Data Sharing",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "OWNERSHIP on tables",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3225
      },
      {
        "answerText": "USAGE on database",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3222
      },
      {
        "answerText": "MANAGE SHARES",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3224
      },
      {
        "answerText": "CREATE SHARE",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3223
      }
    ]
  },
  {
    "id": 586,
    "questionText": "What does the SHOW command do in Snowflake?",
    "questionType": "single",
    "explanation": "The SHOW command in Snowflake lists objects of a specified type (SHOW TABLES, SHOW DATABASES, SHOW ROLES, SHOW WAREHOUSES, etc.) along with their properties and metadata. Results are filtered based on the current role privileges - you only see objects your role has access to. SHOW commands return metadata like object names, owners, creation dates, and type-specific properties.\n\nWhy other options fail:\n\u2022 Display data from a table - use SELECT to query table data; SHOW returns metadata about objects, not data within them\n\u2022 Show query execution plan - use EXPLAIN to view query execution plans, not SHOW\n\u2022 Display error logs - error information is found in QUERY_HISTORY views or VALIDATE function output, not SHOW commands\n\nKey exam tip: SHOW returns metadata about objects; SELECT returns data from tables. Common SHOW variants include SHOW TABLES, SHOW SCHEMAS, SHOW DATABASES, SHOW ROLES, SHOW GRANTS, and SHOW WAREHOUSES.",
    "domain": "1",
    "topic": "Commands",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "Show query execution plan",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3228
      },
      {
        "answerText": "List objects and their properties",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3227
      },
      {
        "answerText": "Display error logs",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3229
      },
      {
        "answerText": "Display data from a table",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3226
      }
    ]
  },
  {
    "id": 587,
    "questionText": "Which Snowflake feature allows creating secure, parameterized data products?",
    "questionType": "single",
    "explanation": "Secure UDFs (User-Defined Functions) are the key feature for creating secure, parameterized data products in Snowflake. When shared, Secure UDFs hide their implementation logic from consumers while allowing them to pass parameters to access specific data subsets. This enables providers to share functions that enforce business rules, apply row-level filtering, or compute derived values - all without exposing underlying table structures or query logic.\n\nWhy other options fail:\n\u2022 Dynamic Tables - these provide automated incremental materialization of query results, not parameterized secure access for data sharing\n\u2022 Stored Procedures - while they can be shared, they execute actions rather than return query results, and are less suited for parameterized data product access patterns\n\u2022 API Integrations - these connect Snowflake to external services but do not create shareable parameterized data products within Snowflake\n\nKey exam tip: Secure UDFs + Secure Views are the foundation for creating data products on Snowflake Marketplace. The Secure designation hides DDL/logic from consumers while maintaining functionality.",
    "domain": "2",
    "topic": "Secure UDFs",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Secure UDFs",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3231
      },
      {
        "answerText": "Stored Procedures",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3232
      },
      {
        "answerText": "Dynamic Tables",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3230
      },
      {
        "answerText": "API Integrations",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3233
      }
    ]
  },
  {
    "id": 588,
    "questionText": "What is the minimum billing increment for Snowflake compute?",
    "questionType": "single",
    "explanation": "Snowflake compute billing uses a 60-second (1-minute) minimum charge, after which billing continues in 1-second increments. This means when a warehouse starts, you pay for at least one minute even if your query takes only 5 seconds. After that first minute, additional usage is billed per-second until the warehouse suspends. This model encourages efficient warehouse sizing and auto-suspend configuration.\n\nWhy other options fail:\n\u2022 Per-hour billing only - this was the traditional data warehouse model; Snowflake per-second billing (after 1-min minimum) is more granular and cost-effective\n\u2022 5-minute increments - this is incorrect; Snowflake uses 1-second increments after the initial 60-second minimum\n\u2022 Per-query billing - standard virtual warehouses bill for runtime, not per-query; Snowflake does offer serverless features with per-query billing but that is not the standard compute model\n\nKey exam tip: Remember 60-second minimum, then per-second for warehouse billing. This is why AUTO_SUSPEND and AUTO_RESUME are critical for cost optimization.",
    "domain": "1",
    "topic": "Billing",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "5-minute increments",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3236
      },
      {
        "answerText": "Per-hour billing only",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3235
      },
      {
        "answerText": "Per-query billing",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3237
      },
      {
        "answerText": "1 minute minimum, then per-second",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3234
      }
    ]
  },
  {
    "id": 589,
    "questionText": "Which Snowflake view provides information about query execution history?",
    "questionType": "single",
    "explanation": "QUERY_HISTORY is the view that provides comprehensive query execution history in Snowflake. It exists in two locations: INFORMATION_SCHEMA.QUERY_HISTORY (real-time, 7-day retention, current session context) and SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY (45-minute latency, 365-day retention, account-wide). These views contain detailed metrics including execution time, bytes scanned, rows produced, warehouse used, query text, and compilation time.\n\nWhy other options fail:\n\u2022 QUERY_LOG - this is not a valid Snowflake view name; the correct name is QUERY_HISTORY\n\u2022 EXECUTION_HISTORY - this view does not exist in Snowflake; query execution data is in QUERY_HISTORY\n\u2022 QUERY_AUDIT - not a Snowflake view; audit-related information is captured in ACCESS_HISTORY and LOGIN_HISTORY views\n\nKey exam tip: INFORMATION_SCHEMA views are real-time but limited to 7 days and current context. ACCOUNT_USAGE views have 45-min to 3-hour latency but retain data for up to 365 days with account-wide scope.",
    "domain": "3",
    "topic": "Monitoring",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "QUERY_AUDIT",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3241
      },
      {
        "answerText": "QUERY_LOG",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3238
      },
      {
        "answerText": "QUERY_HISTORY",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3239
      },
      {
        "answerText": "EXECUTION_HISTORY",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3240
      }
    ]
  },
  {
    "id": 590,
    "questionText": "What is the purpose of the ON_ERROR option in COPY INTO?",
    "questionType": "single",
    "explanation": "The ON_ERROR option in COPY INTO controls how Snowflake handles errors encountered during data loading. It determines whether to continue loading despite errors or stop the operation. Available values are: CONTINUE (skip error rows, continue loading), SKIP_FILE (skip entire file if any error occurs), SKIP_FILE_<num> (skip file after specified number of errors), SKIP_FILE_<num>% (skip file after error percentage threshold), and ABORT_STATEMENT (default - stop loading on first error).\n\nWhy other options fail:\n\u2022 Log errors to a table - error logging is controlled by the separate VALIDATION_MODE option or by querying the COPY command results, not ON_ERROR\n\u2022 Send error notifications - Snowflake does not have built-in error notifications in COPY; you would need external alerting via tasks or third-party tools\n\u2022 Retry failed operations - COPY does not automatically retry; it either continues, skips, or aborts based on ON_ERROR setting\n\nKey exam tip: Default ON_ERROR is ABORT_STATEMENT. Use CONTINUE for maximum data loading with error tolerance. Combine with VALIDATION_MODE = RETURN_ERRORS to preview errors before actual loading.",
    "domain": "4",
    "topic": "Error Handling",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Define error handling behavior during load",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3243
      },
      {
        "answerText": "Log errors to a table",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3242
      },
      {
        "answerText": "Retry failed operations",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3245
      },
      {
        "answerText": "Send error notifications",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3244
      }
    ]
  },
  {
    "id": 591,
    "questionText": "Which Snowflake feature enables running containerized workloads?",
    "questionType": "single",
    "explanation": "Snowpark Container Services (SPCS) enables running containerized workloads directly within Snowflake secure environment. It supports OCI-compliant Docker containers, allowing deployment of custom applications, ML models, web services, and specialized processing engines. SPCS uses Compute Pools (managed GPU/CPU clusters) to run containers, with native integration to Snowflake data - eliminating the need to move data outside Snowflake for processing.\n\nWhy other options fail:\n\u2022 Snowpark Functions - these are UDFs written in Python/Java/Scala that run in Snowflake execution environment, not arbitrary containerized workloads\n\u2022 External Functions - these call external API endpoints hosted outside Snowflake; they do not run containers within Snowflake\n\u2022 Snowpark Client Libraries - these are language SDKs (Python, Java, Scala) for building Snowpark applications, not a container runtime service\n\nKey exam tip: Snowpark Container Services = containers running INSIDE Snowflake. External Functions = calling services OUTSIDE Snowflake. SPCS requires Compute Pools for container execution.",
    "domain": "1",
    "topic": "Container Services",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "External Functions",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3248
      },
      {
        "answerText": "Snowpark Container Services",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3247
      },
      {
        "answerText": "Snowpark Functions",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3246
      },
      {
        "answerText": "Snowpark Client Libraries",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3249
      }
    ]
  },
  {
    "id": 592,
    "questionText": "What is the purpose of the PIPE_EXECUTION_PAUSED status in Snowpipe?",
    "questionType": "single",
    "explanation": "PIPE_EXECUTION_PAUSED is a Snowpipe status indicating that the pipe automated data loading has been temporarily suspended. This status occurs when: (1) an administrator explicitly pauses the pipe using ALTER PIPE ... SET PIPE_EXECUTION_PAUSED = TRUE, (2) the pipe owner role loses necessary privileges, or (3) underlying objects (stage, table) become unavailable. While paused, new files are still tracked but not loaded until the pipe is resumed.\n\nWhy other options fail:\n\u2022 Pipe is processing files - an actively loading pipe would show as RUNNING or have load history, not PAUSED status\n\u2022 Pipe encountered an error - errors during loading do not automatically pause the pipe; they are logged in COPY_HISTORY and the pipe continues watching for new files\n\u2022 Pipe is waiting for files - a pipe waiting for new files is in normal operational state, not paused; it is ready to load when files arrive\n\nKey exam tip: Use ALTER PIPE ... SET PIPE_EXECUTION_PAUSED = FALSE to resume a paused pipe. Check SYSTEM$PIPE_STATUS() for current pipe state and PIPE_USAGE_HISTORY for loading activity.",
    "domain": "4",
    "topic": "Snowpipe",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Pipe execution is temporarily suspended",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3251
      },
      {
        "answerText": "Pipe encountered an error",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3252
      },
      {
        "answerText": "Pipe is waiting for files",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3253
      },
      {
        "answerText": "Pipe is processing files",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3250
      }
    ]
  },
  {
    "id": 593,
    "questionText": "Which function is used to parse JSON strings into VARIANT type?",
    "questionType": "single",
    "explanation": "PARSE_JSON is the Snowflake function that converts a JSON-formatted string into VARIANT data type, enabling semi-structured data processing. It validates the JSON syntax during parsing and returns a VARIANT value that can be queried using dot notation (data:key) or bracket notation (data[key]). This function is essential when JSON data is stored as VARCHAR and needs to be processed as structured data.\n\nWhy other options fail:\n\u2022 JSON_PARSE - this function name does not exist in Snowflake; the correct function is PARSE_JSON\n\u2022 TO_JSON - this function does the opposite: it converts a VARIANT value back to a JSON string, not parse a string into VARIANT\n\u2022 CONVERT_JSON - this is not a valid Snowflake function; use PARSE_JSON for string-to-VARIANT conversion\n\nKey exam tip: PARSE_JSON converts string to VARIANT; TO_JSON converts VARIANT to string. For direct JSON loading, use VARIANT columns where Snowflake auto-parses JSON during COPY INTO.",
    "domain": "5",
    "topic": "JSON Functions",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "CONVERT_JSON",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3257
      },
      {
        "answerText": "PARSE_JSON",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3255
      },
      {
        "answerText": "TO_JSON",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3256
      },
      {
        "answerText": "JSON_PARSE",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3254
      }
    ]
  },
  {
    "id": 594,
    "questionText": "What is the purpose of replication groups in Snowflake?",
    "questionType": "single",
    "explanation": "Replication groups in Snowflake allow you to replicate multiple account objects (databases, shares, users, roles, warehouses, etc.) together as a single, consistent unit across regions and cloud providers. This ensures transactional consistency - all objects in the group are replicated to the same point in time. Replication groups are essential for disaster recovery, enabling failover to secondary regions while maintaining data and access control consistency.\n\nWhy other options fail:\n\u2022 Group warehouses for billing - warehouse billing is managed through resource monitors and account billing, not replication groups\n\u2022 Group users for access control - roles and role hierarchies manage user access grouping; replication groups handle cross-region replication\n\u2022 Organize databases logically - logical organization is achieved through naming conventions and schemas; replication groups serve disaster recovery and data distribution purposes\n\nKey exam tip: Replication groups support account-level replication (users, roles, warehouses) beyond just database replication. They enable true disaster recovery with consistent state across all replicated objects.",
    "domain": "6",
    "topic": "Replication",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Organize databases logically",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3261
      },
      {
        "answerText": "Group warehouses for billing",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3258
      },
      {
        "answerText": "Replicate objects across regions as a unit",
        "isCorrect": true,
        "sortOrder": 2,
        "id": 3259
      },
      {
        "answerText": "Group users for access control",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3260
      }
    ]
  },
  {
    "id": 595,
    "questionText": "Which Snowflake function returns the account identifier?",
    "questionType": "single",
    "explanation": "CURRENT_ACCOUNT() is a context function that returns the account locator (the unique identifier assigned to your Snowflake account) for the current session. This is useful for logging, auditing, and multi-account scenarios where you need to identify which account is executing queries. Related functions include CURRENT_ACCOUNT_NAME() for the account name and CURRENT_ORGANIZATION_NAME() for the organization.\n\nWhy other options fail:\n\u2022 GET_ACCOUNT() - this function does not exist in Snowflake; context functions use the CURRENT_ prefix convention\n\u2022 ACCOUNT_ID() - not a valid Snowflake function; the correct function is CURRENT_ACCOUNT()\n\u2022 SESSION_ACCOUNT() - this function does not exist; while it correctly implies session context, the actual function is CURRENT_ACCOUNT()\n\nKey exam tip: Context functions all start with CURRENT_: CURRENT_USER(), CURRENT_ROLE(), CURRENT_WAREHOUSE(), CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_SESSION(), CURRENT_ACCOUNT().",
    "domain": "5",
    "topic": "Context Functions",
    "difficulty": "easy",
    "answers": [
      {
        "answerText": "GET_ACCOUNT()",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3262
      },
      {
        "answerText": "CURRENT_ACCOUNT()",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3263
      },
      {
        "answerText": "SESSION_ACCOUNT()",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3265
      },
      {
        "answerText": "ACCOUNT_ID()",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3264
      }
    ]
  },
  {
    "id": 596,
    "questionText": "What is the purpose of the STATEMENT_TIMEOUT_IN_SECONDS parameter?",
    "questionType": "single",
    "explanation": "STATEMENT_TIMEOUT_IN_SECONDS is a parameter that defines the maximum execution time (in seconds) for a SQL statement before Snowflake automatically cancels it. This protects against runaway queries consuming resources indefinitely. The parameter can be set at multiple levels: account (default for all users), user (overrides account default), session (overrides user default), or even object level. Value of 0 means no timeout.\n\nWhy other options fail:\n\u2022 Session idle timeout - this is controlled by CLIENT_SESSION_KEEP_ALIVE and related parameters, not STATEMENT_TIMEOUT_IN_SECONDS\n\u2022 Transaction timeout - while long-running transactions may trigger statement timeouts, LOCK_TIMEOUT handles transaction locking; STATEMENT_TIMEOUT is specifically for query execution duration\n\u2022 Connection timeout - connection-level timeouts are managed by client drivers and network settings, not this Snowflake parameter\n\nKey exam tip: Use STATEMENT_TIMEOUT_IN_SECONDS for query governance. Combine with STATEMENT_QUEUED_TIMEOUT_IN_SECONDS (time waiting in queue) for comprehensive query management. Both help prevent resource abuse.",
    "domain": "3",
    "topic": "Query Management",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Maximum query execution time before cancellation",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3267
      },
      {
        "answerText": "Connection timeout",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3269
      },
      {
        "answerText": "Transaction timeout",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3268
      },
      {
        "answerText": "Session idle timeout",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3266
      }
    ]
  },
  {
    "id": 597,
    "questionText": "Which type of constraint is enforced by Snowflake?",
    "questionType": "single",
    "explanation": "NOT NULL is the only constraint type that Snowflake actively enforces at data modification time. When a column has a NOT NULL constraint, Snowflake will reject any INSERT or UPDATE that attempts to place a NULL value in that column. All other constraints (PRIMARY KEY, UNIQUE, FOREIGN KEY) are declarative/informational only - they are stored in metadata for documentation and query optimization hints but do not prevent duplicate or invalid data.\n\nWhy other options fail:\n\u2022 PRIMARY KEY - Snowflake does NOT enforce primary key uniqueness; duplicate values can be inserted without error. The constraint is informational only\n\u2022 FOREIGN KEY - referential integrity is NOT enforced; you can insert child records with no matching parent. Foreign keys are metadata for documentation and BI tools\n\u2022 UNIQUE - uniqueness is NOT enforced; duplicate values are allowed even with UNIQUE constraints defined\n\nKey exam tip: This is a frequently tested concept! Only NOT NULL is enforced. The rationale is that Snowflake architecture prioritizes high-speed bulk loading over transactional constraint checking. Use data quality tools or application logic to enforce other constraints.",
    "domain": "5",
    "topic": "Constraints",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "FOREIGN KEY",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3272
      },
      {
        "answerText": "UNIQUE",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3273
      },
      {
        "answerText": "PRIMARY KEY",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3270
      },
      {
        "answerText": "NOT NULL",
        "isCorrect": true,
        "sortOrder": 3,
        "id": 3271
      }
    ]
  },
  {
    "id": 598,
    "questionText": "What is the purpose of federated authentication in Snowflake?",
    "questionType": "single",
    "explanation": "Federated authentication in Snowflake enables users to authenticate via an external Identity Provider (IdP) using SAML 2.0 protocol, providing Single Sign-On (SSO) capabilities. Users authenticate against their organization IdP (such as Okta, Azure AD, or ADFS) rather than maintaining separate Snowflake credentials. This centralizes identity management, enforces organizational security policies, and simplifies user access across multiple systems.\n\nWhy other options fail:\n\u2022 Authenticate across multiple Snowflake accounts - cross-account access uses organization-level features and account linking, not federated authentication\n\u2022 Combine multiple authentication methods - Snowflake supports multiple auth methods (password, SSO, key pair) but federated specifically means delegating to external IdP via SAML\n\u2022 Share authentication credentials between users - sharing credentials is a security anti-pattern; federated auth provides individual user authentication through a central IdP\n\nKey exam tip: Federated authentication requires SAML 2.0 IdP integration configured by ACCOUNTADMIN. Users can still have local Snowflake passwords as backup unless explicitly disabled.",
    "domain": "2",
    "topic": "Authentication",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "Use external identity provider for authentication",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3275
      },
      {
        "answerText": "Authenticate across multiple Snowflake accounts",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3274
      },
      {
        "answerText": "Combine multiple authentication methods",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3276
      },
      {
        "answerText": "Share authentication credentials between users",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3277
      }
    ]
  },
  {
    "id": 599,
    "questionText": "Which Snowflake command refreshes an external table's metadata?",
    "questionType": "single",
    "explanation": "ALTER EXTERNAL TABLE ... REFRESH is the command that synchronizes an external table metadata with the current state of files in its referenced external stage. When files are added, modified, or deleted in cloud storage (S3, Azure Blob, GCS), this command updates the external table partitions and file tracking. Refresh can be triggered manually or automatically configured with cloud event notifications (SNS, Event Grid, Pub/Sub).\n\nWhy other options fail:\n\u2022 REFRESH EXTERNAL TABLE - incorrect syntax; REFRESH is a clause within ALTER EXTERNAL TABLE, not a standalone command\n\u2022 UPDATE EXTERNAL TABLE - UPDATE is a DML command for modifying data; external tables are read-only and use ALTER for metadata operations\n\u2022 SYNC EXTERNAL TABLE - SYNC is not a valid Snowflake command; the correct syntax uses ALTER ... REFRESH\n\nKey exam tip: External tables require REFRESH to see new files. For automation, configure AUTO_REFRESH = TRUE with cloud event notifications. Manual refresh: ALTER EXTERNAL TABLE my_ext_table REFRESH;",
    "domain": "4",
    "topic": "External Tables",
    "difficulty": "medium",
    "answers": [
      {
        "answerText": "SYNC EXTERNAL TABLE",
        "isCorrect": false,
        "sortOrder": 0,
        "id": 3281
      },
      {
        "answerText": "ALTER EXTERNAL TABLE ... REFRESH",
        "isCorrect": true,
        "sortOrder": 1,
        "id": 3279
      },
      {
        "answerText": "REFRESH EXTERNAL TABLE",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3278
      },
      {
        "answerText": "UPDATE EXTERNAL TABLE",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3280
      }
    ]
  },
  {
    "id": 600,
    "questionText": "What is the purpose of the ORGADMIN role in Snowflake?",
    "questionType": "single",
    "explanation": "ORGADMIN is a system-defined role that manages organization-level settings and operations across all accounts within a Snowflake organization. Key responsibilities include: creating and managing Snowflake accounts, enabling replication and failover across accounts, viewing organization-wide usage and billing, managing organization-level features, and configuring organization policies. ORGADMIN operates at a level above individual accounts.\n\nWhy other options fail:\n\u2022 Manage databases across accounts - database management is done by roles within each account (SYSADMIN, database owners); ORGADMIN manages accounts themselves, not their contents\n\u2022 Organize users into groups - user grouping is handled by roles within accounts; ORGADMIN manages account-level administration, not user-level access control\n\u2022 Manage organizational hierarchy - this is vague and does not reflect ORGADMIN actual purpose of account administration and organization-wide settings\n\nKey exam tip: System roles hierarchy from highest to lowest: ORGADMIN (org level) > ACCOUNTADMIN > SECURITYADMIN/SYSADMIN > USERADMIN > PUBLIC. ORGADMIN is the only role that operates across accounts within an organization.",
    "domain": "2",
    "topic": "System Roles",
    "difficulty": "hard",
    "answers": [
      {
        "answerText": "Manage organization-level settings and accounts",
        "isCorrect": true,
        "sortOrder": 0,
        "id": 3283
      },
      {
        "answerText": "Organize users into groups",
        "isCorrect": false,
        "sortOrder": 1,
        "id": 3284
      },
      {
        "answerText": "Manage databases across accounts",
        "isCorrect": false,
        "sortOrder": 2,
        "id": 3282
      },
      {
        "answerText": "Manage organizational hierarchy",
        "isCorrect": false,
        "sortOrder": 3,
        "id": 3285
      }
    ]
  }
]